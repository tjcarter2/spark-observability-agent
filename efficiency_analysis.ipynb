{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a60d1bb-706e-49e6-8f2b-f735826597c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Efficiency Analysis\n",
    "These engine agnostic queries analyze spark history server task summaries and stages to surface inefficient spark applications based on skew, spill, partition size, processing rate, and task failure rates. The sink tables are optimized to allow for traditional sql based analytics or agentic natural language analysis as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62335bd6-6aa0-4995-8a2f-eef36fe890b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or \"spark_observability\"\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11e6f6b-89ec-4f73-acb1-964d83b7068f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ineffstageagg\":153},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758043494431}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sql\n",
    "-- diff of more than 2x btwn min and median or median and max\n",
    "-- any spill\n",
    "-- chunk sizes\n",
    "-- diff of more than 2x processing rate\n",
    "-- with task completion rate\n",
    "create or replace table IDENTIFIER(:catalog_name || '.' || :schema_name || '.' || 'ineffjobagg')\n",
    "as\n",
    "with rawmetrics as (\n",
    "select *, try_variant_get(parse_json(raw_json),'$.duration', 'array<double>') as duration, try_variant_get(parse_json(raw_json),'$.memoryBytesSpilled', 'array<double>') as memoryBytesSpilled, try_variant_get(parse_json(raw_json),'$.diskBytesSpilled', 'array<double>') as diskBytesSpilled, try_variant_get(parse_json(raw_json),'$.inputMetrics.bytesRead', 'array<double>') as bytesRead , \n",
    "try_variant_get(parse_json(raw_json),'$.outputMetrics.bytesWritten', 'array<double>') as bytesWritten , try_variant_get(parse_json(raw_json),'$.shuffleReadMetrics.readBytes', 'array<double>') readBytes,\n",
    "try_variant_get(parse_json(raw_json),'$.shuffleReadMetrics.fetchWaitTime', 'array<double>') fetchWaitTime,\n",
    "try_variant_get(parse_json(raw_json),'$.shuffleWriteMetrics.writeBytes', 'array<double>') writeBytes\n",
    "from prodrs.spark_observability.task_summaries),\n",
    "\n",
    "polmetrics as (select *, duration[0] as min, duration[2] as median, duration[4] as maxim, memoryBytesSpilled[2] as spillmed, memoryBytesSpilled[4] as spillmax, diskBytesSpilled[2] as diskspillmed, diskBytesSpilled[4] as diskspillmax, bytesRead[0] as minbytes, bytesRead[2] as medbytes, bytesRead[4] as maxbytes, bytesWritten[0] as minbyteswr, bytesWritten[2] as medbyteswr, bytesWritten[4] as maxbyteswr, readbytes[0] as minsr, readbytes[2] as medsr, readbytes[4] as maxsr, writebytes[0] as minsw, writebytes[2] as medsw, writebytes[4] as maxsw,\n",
    "map('minbytes', minbytes, 'minbyteswr', minbyteswr, 'minsr', minsr, 'minsw',minsw) as minchunks,\n",
    "map('minbytes', medbytes, 'medbyteswr', medbyteswr, 'medsr', medsr, 'medsw',medsw) as medchunks,\n",
    "map('maxbytes', maxbytes, 'maxbyteswr', maxbyteswr, 'maxsr', maxsr, 'maxsw',maxsw) as maxchunks,\n",
    "map('minbrpr', try_divide(minbytes, min), 'minbwpr', try_divide(minbyteswr, min), 'minsrpr', try_divide(minsr, min), 'minswpr', try_divide(minsw, min)) as minpr,\n",
    "map('medbrpr', try_divide(medbytes, median), 'medbwpr', try_divide(medbyteswr, median), 'medsrpr', try_divide(medsr, median), 'medswpr', try_divide(medsw, median)) as medpr,\n",
    "map('maxbrpr', try_divide(maxbytes, maxim), 'maxbwpr', try_divide(maxbytes, maxim), 'maxsrpr', try_divide(maxsr, maxim), 'maxswpr', try_divide(maxsw, maxim)) as maxpr\n",
    "from rawmetrics),\n",
    "\n",
    "mapping as (select *, diskspillmed, diskspillmax,  map_filter(minchunks, (k, v) -> v > 512000000) as minchunkmap, map_filter(minchunks, (k, v) -> v > 512000000) as medchunkmap, map_filter(minchunks, (k, v) -> v > 512000000) as maxchunkmap,\n",
    "map_filter(minpr, (k, v) -> v between 0.01 and  10000) as minprmap, map_filter(medpr, (k, v) -> v between 0.01 and 10000) as medprmap, map_filter(maxpr, (k, v) -> v between 0.01 and 10000) as maxprmap\n",
    "from polmetrics),\n",
    "\n",
    "precheck as (select cluster_id, cluster_name, stage_id, diskspillmed, diskspillmax, min, median, maxim, spillmed, spillmax, cardinality(minchunkmap) as minchunks, cardinality(medchunkmap) as medchunks, cardinality(maxchunkmap) as maxchunks,\n",
    "cardinality(minprmap) as minpr, cardinality(medprmap) as medpr, cardinality(maxprmap) as maxpr\n",
    "from mapping),\n",
    "\n",
    "goldcheck as (select cluster_id, cluster_name, stage_id, CASE WHEN (median > (2 * min) OR maxim > (2 * median))  THEN 1 ELSE 0 END as skewbinary,\n",
    "CASE WHEN (spillmed > 256000000 or spillmax > 256000000) THEN 1 ELSE 0 END as spillbinary,\n",
    "CASE WHEN (diskspillmed > 256000000 or diskspillmax > 256000000) THEN 1 ELSE 0 END as diskspillbinary,\n",
    "CASE WHEN (minchunks > 0 or medchunks > 0 or maxchunks > 0) THEN 1 ELSE 0 END as chunkbinary,\n",
    "CASE WHEN (minpr > 0 or medpr > 0 or maxpr > 0) THEN 1 ELSE 0 END as prbinary\n",
    "from precheck),\n",
    "\n",
    "enrichjoin as (\n",
    "  select goldcheck.*, cardinality(map_filter(map('skewbinary', skewbinary, 'spillbinary', spillbinary, 'diskspillbinary', diskspillbinary, 'chunkbinary', chunkbinary, 'trbinary', case when stages.task_completion_rate < 0.9 THEN 1 ELSE 0 END ), (k, v) -> v > 0)) as goldcheckfilter, stages.task_completion_rate, timestampdiff(millisecond, stages.submission_time, stages.completion_time) as duration\n",
    "  from goldcheck\n",
    "  join prodrs.spark_observability.stages as stages\n",
    "  on goldcheck.cluster_id = stages.cluster_id\n",
    "  and goldcheck.stage_id = stages.stage_id\n",
    "),\n",
    "\n",
    "pu as (select cluster_id, cluster_name, stage_id, sum(case when goldcheckfilter > 0 then duration else 0 end) as ineffstag,\n",
    "sum(duration) as stageagg\n",
    "from enrichjoin\n",
    "group by all),\n",
    "\n",
    "finalsa as (select cluster_id, cluster_name, try_divide(sum(ineffstag), sum(stageagg)) * 100 as ineffstageagg\n",
    "from pu\n",
    "group by all),\n",
    "\n",
    "finaljoin as(\n",
    "  select finalsa.*, apps.start_time\n",
    "  from finalsa\n",
    "  join prodrs.spark_observability.applications apps\n",
    "  on finalsa.cluster_id = apps.cluster_id\n",
    ")\n",
    "\n",
    "select * from finaljoin\n",
    "where ineffstageagg > 25\n",
    "--order by 3 desc, 4 desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e10cda-52d2-47e9-9f6b-c0aa9ed75710",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764960808805}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %sql\n",
    "-- diff of more than 2x btwn min and median or median and max\n",
    "-- any spill\n",
    "-- chunk sizes\n",
    "-- diff of more than 2x processing rate\n",
    "-- with task completion rate\n",
    "\n",
    "create or replace table IDENTIFIER(:catalog_name || '.' || :schema_name || '.' || 'ineffjobraw')\n",
    "as\n",
    "with rawmetrics as (\n",
    "select *, try_variant_get(parse_json(raw_json),'$.duration', 'array<double>') as duration, try_variant_get(parse_json(raw_json),'$.memoryBytesSpilled', 'array<double>') as memoryBytesSpilled, try_variant_get(parse_json(raw_json),'$.diskBytesSpilled', 'array<double>') as diskBytesSpilled, try_variant_get(parse_json(raw_json),'$.inputMetrics.bytesRead', 'array<double>') as bytesRead , \n",
    "try_variant_get(parse_json(raw_json),'$.outputMetrics.bytesWritten', 'array<double>') as bytesWritten , try_variant_get(parse_json(raw_json),'$.shuffleReadMetrics.readBytes', 'array<double>') readBytes,\n",
    "try_variant_get(parse_json(raw_json),'$.shuffleReadMetrics.fetchWaitTime', 'array<double>') fetchWaitTime,\n",
    "try_variant_get(parse_json(raw_json),'$.shuffleWriteMetrics.writeBytes', 'array<double>') writeBytes\n",
    "from prodrs.spark_observability.task_summaries),\n",
    "\n",
    "polmetrics as (select *, duration[0] as min, duration[2] as median, duration[4] as maxim, memoryBytesSpilled[2] as spillmed, memoryBytesSpilled[4] as spillmax, diskBytesSpilled[2] as diskspillmed, diskBytesSpilled[4] as diskspillmax, bytesRead[0] as minbytes, bytesRead[2] as medbytes, bytesRead[4] as maxbytes, bytesWritten[0] as minbyteswr, bytesWritten[2] as medbyteswr, bytesWritten[4] as maxbyteswr, readbytes[0] as minsr, readbytes[2] as medsr, readbytes[4] as maxsr, writebytes[0] as minsw, writebytes[2] as medsw, writebytes[4] as maxsw,\n",
    "map('minbytes', minbytes, 'minbyteswr', minbyteswr, 'minsr', minsr, 'minsw',minsw) as minchunks,\n",
    "map('minbytes', medbytes, 'medbyteswr', medbyteswr, 'medsr', medsr, 'medsw',medsw) as medchunks,\n",
    "map('maxbytes', maxbytes, 'maxbyteswr', maxbyteswr, 'maxsr', maxsr, 'maxsw',maxsw) as maxchunks,\n",
    "map('minbrpr', try_divide(minbytes, min), 'minbwpr', try_divide(minbyteswr, min), 'minsrpr', try_divide(minsr, min), 'minswpr', try_divide(minsw, min)) as minpr,\n",
    "map('medbrpr', try_divide(medbytes, median), 'medbwpr', try_divide(medbyteswr, median), 'medsrpr', try_divide(medsr, median), 'medswpr', try_divide(medsw, median)) as medpr,\n",
    "map('maxbrpr', try_divide(maxbytes, maxim), 'maxbwpr', try_divide(maxbytes, maxim), 'maxsrpr', try_divide(maxsr, maxim), 'maxswpr', try_divide(maxsw, maxim)) as maxpr\n",
    "from rawmetrics),\n",
    "\n",
    "mapping as (select *, diskspillmed, diskspillmax,  map_filter(minchunks, (k, v) -> v > 512000000) as minchunkmap, map_filter(minchunks, (k, v) -> v > 512000000) as medchunkmap, map_filter(minchunks, (k, v) -> v > 512000000) as maxchunkmap,\n",
    "map_filter(minpr, (k, v) -> v between 0.01 and  10000) as minprmap, map_filter(medpr, (k, v) -> v between 0.01 and 10000) as medprmap, map_filter(maxpr, (k, v) -> v between 0.01 and 10000) as maxprmap\n",
    "from polmetrics),\n",
    "\n",
    "precheck as (select cluster_id, cluster_name, stage_id, diskspillmed, diskspillmax, min, median, maxim, spillmed, spillmax, cardinality(minchunkmap) as minchunks, cardinality(medchunkmap) as medchunks, cardinality(maxchunkmap) as maxchunks,\n",
    "cardinality(minprmap) as minpr, cardinality(medprmap) as medpr, cardinality(maxprmap) as maxpr\n",
    "from mapping),\n",
    "\n",
    "goldcheck as (select cluster_id, cluster_name, stage_id, CASE WHEN (median > (2 * min) OR maxim > (2 * median))  THEN 1 ELSE 0 END as skewbinary,\n",
    "CASE WHEN (spillmed > 256000000 or spillmax > 256000000) THEN 1 ELSE 0 END as spillbinary,\n",
    "CASE WHEN (diskspillmed > 256000000 or diskspillmax > 256000000) THEN 1 ELSE 0 END as diskspillbinary,\n",
    "CASE WHEN (minchunks > 0 or medchunks > 0 or maxchunks > 0) THEN 1 ELSE 0 END as chunkbinary,\n",
    "CASE WHEN (minpr > 0 or medpr > 0 or maxpr > 0) THEN 1 ELSE 0 END as prbinary\n",
    "from precheck),\n",
    "\n",
    "enrichjoin as (\n",
    "  select goldcheck.*, cardinality(map_filter(map('skewbinary', skewbinary, 'spillbinary', spillbinary, 'diskspillbinary', diskspillbinary, 'chunkbinary', chunkbinary, 'trbinary', case when stages.task_completion_rate < 0.9 THEN 1 ELSE 0 END ), (k, v) -> v > 0)) as goldcheckfilter, stages.task_completion_rate, timestampdiff(millisecond, stages.submission_time, stages.completion_time) as duration, stages.completion_time\n",
    "  from goldcheck\n",
    "  join prodrs.spark_observability.stages as stages\n",
    "  on goldcheck.cluster_id = stages.cluster_id\n",
    "  and goldcheck.stage_id = stages.stage_id\n",
    ")\n",
    "\n",
    "select * from enrichjoin\n",
    "--where cluster_id = '1108-063956-zfgrta8i'\n",
    "--where ineffstageagg > 25\n",
    "--order by 3 desc, 4 desc"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "efficiency_analysis_prod",
   "widgets": {
    "catalog_name": {
     "currentValue": "prodrs",
     "nuid": "a5234975-1a30-497a-8b2f-f2150dd360e2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "spark_observability",
     "nuid": "070669a3-2563-4043-b7c3-edb5b98c745b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

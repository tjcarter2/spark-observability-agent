{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88aee1a8-c986-42cb-b6c8-5bf76fc6255b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or \"spark_observability\"\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdafbfe-9ae3-4448-9021-816b42b2599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG IDENTIFIER(:catalog_name);\n",
    "USE SCHEMA IDENTIFIER(:schema_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da65e6c-f9a0-434a-b8f3-d968f026ed65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION sqlmetrics(\n",
    "  clusterid string, limit double default 20000, ranking double DEFAULT 0 \n",
    ")\n",
    "\n",
    "RETURNS TABLE (nodestring STRING, successJobIds array<string>, stringlength string, rank double)\n",
    "COMMENT 'this tool returns string snippet of node metrics for a spark sql query, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(listshssqlraw(clusterid))::array<struct<id:long, status:string, description:string, planDescription:string, submissionTime:string, duration:long, successJobIds:array<string>, failedJobIds:string, nodes: array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>> >> as sqlmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(sqlmetrics) as sqlmetricsexp\n",
    "  from raw\n",
    "),\n",
    "\n",
    "pu as (select to_json(sqlmetricsexp.nodes) as nodestring, sqlmetricsexp.successJobIds as jobids,\n",
    "len(to_binary(to_json(sqlmetricsexp.nodes), \"UTF-8\")) as stringlength,\n",
    "rank() over (order by len(to_binary(to_json(sqlmetricsexp.nodes), \"UTF-8\")) desc) as rank\n",
    "from explode)\n",
    "\n",
    "select nodestring, jobids, stringlength, rank\n",
    "from pu \n",
    "where stringlength < limit\n",
    "and if(ranking = 0, 1 = 1, rank = ranking)\n",
    "order by stringlength desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d39b2a9-3429-450b-a7b4-fea0cb7c5058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION jobmetrics(\n",
    "  clusterid string, jobidsarr array<string>\n",
    ")\n",
    "\n",
    "RETURNS TABLE (jobId string, name string, description string, submissionTime string, completionTime string, stageIds array<string>, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, runtimesec long)\n",
    "COMMENT 'this tool returns spark history server job metrics for a spark applications, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(listshsjobsraw(clusterid))::array<struct<jobId:string, name:string, description:string, submissionTime:string, completionTime:string, stageIds:array<string>, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double>> as jobmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(jobmetrics) as jobmetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select jobmetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(jobmetricsexp.submissionTime), to_timestamp(jobmetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "where array_contains(jobidsarr, jobmetricsexp.jobid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f71b207-4dc0-481e-acd1-31545536b137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION stagemetrics(\n",
    "  clusterid string, stageidsarr array<string>\n",
    ")\n",
    "\n",
    "RETURNS TABLE (stageId string, attemptId string, name string, description string, submissionTime string, completionTime string, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, memoryBytesSpilled long, diskBytesSpilled long, inputBytes long, inputRecords long, outputBytes long, outputRecords long, shuffleReadBytes long, shuffleReadRecords long, shuffleWriteBytes long, shuffleWriteRecords long, runtimesec long)\n",
    "COMMENT 'this tool returns spark history server stage metrics for a spark applications, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(listshsstagesraw(clusterid))::array<struct<stageId:string, attemptId:string, name:string, description:string, submissionTime:string, completionTime:string, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double, memoryBytesSpilled:long, diskBytesSpilled:long, inputBytes:long, inputRecords:long, outputBytes:long, outputRecords:long, shuffleReadBytes:long, shuffleReadRecords:long, shuffleWriteBytes:long, shuffleWriteRecords:long >> as stagemetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(stagemetrics) as stagemetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select stagemetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(stagemetricsexp.submissionTime), to_timestamp(stagemetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "where array_contains(stageidsarr, stagemetricsexp.stageId) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73754aae-0499-49f7-8be8-f9dde665be22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION taskmetrics(\n",
    "  clusterid string, stageid int\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get tasks list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/stages/%s/0/taskSummary\", clusterid, getsparkcontext(clusterid):spark_context_id, getappid(clusterid), stageid),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7c81b0-4a51-4a0a-8b10-85a7ad861187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Approach 2: Define photon logic upfront, execute within the UDF, have LLM return final value. This is only meant to be directionally accurate. SHS does not expose expressions. Further this is not a weighted average as it treats all operators essentially the same. \n",
    "\n",
    "CREATE OR REPLACE FUNCTION photonmetrics(\n",
    "  clusterid string\n",
    ")\n",
    "\n",
    "RETURNS DOUBLE\n",
    "COMMENT 'Analyzes spark history server sql metrics to derive estimate of how much of spark job would benefit from photon'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(listshssqlraw(clusterid))::array<struct<id:long, status:string, description:string, planDescription:string, submissionTime:string, duration:long, successJobIds:string, failedJobIds:string, nodes: array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>> >> as sqlmetrics),\n",
    "\n",
    "firstexplode as (\n",
    "  select explode(sqlmetrics) as sqlmetricsexp\n",
    "  from raw\n",
    "),\n",
    "\n",
    "secexplode as (\n",
    "  select sqlmetricsexp.*, nodemetrics\n",
    "  from firstexplode\n",
    "  lateral view explode(sqlmetricsexp.nodes) as nodemetrics\n",
    "),\n",
    "\n",
    "photoncheck as (select *, case when nodemetrics.nodeName = 'MapElements' then 0 \n",
    "when nodemetrics.nodeName = 'MapPartitions' then 0 \n",
    "when nodemetrics.nodeName = 'Scan csv' then 0\n",
    "when nodemetrics.nodeName = 'Scan json' then 0 \n",
    "when nodemetrics.nodeName = 'PythonUDF' then 0 \n",
    "when nodemetrics.nodeName = 'ScalaUDF' then 0 \n",
    "when nodemetrics.nodeName = 'FlatMapGroupsInPandas' then 0  \n",
    "when nodemetrics.nodeName = 'DeserializeToObject' then 0\n",
    "when nodemetrics.nodeName = 'SerializeFromObject' then 0  \n",
    "else 1 end as photonbinary\n",
    "from secexplode),\n",
    "\n",
    "jobcheck as (select try_divide(sum(photonbinary), count(*)) as jobphotonperc \n",
    "from photoncheck \n",
    "group by all)\n",
    "\n",
    "select jobphotonperc\n",
    "from jobcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd425024-1418-4da0-a0a4-249126cc062a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION py_listshssqlraw(cluster_id STRING, pat_token STRING, cp_url STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "-- update with your service credential name\n",
    "AS $$\n",
    "\n",
    "import requests, base64, json, re\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "import time \n",
    "\n",
    "def safe_b64_decode(val):\n",
    "    if not val: return val\n",
    "    try: return base64.b64decode(val).decode('utf-8') if not val.startswith(\"http\") else val\n",
    "    except: return val\n",
    "\n",
    "try:\n",
    "    # 1. AUTH & CLIENT\n",
    "    w = WorkspaceClient(host=cp_url, token=pat_token)\n",
    "\n",
    "    # 2. DYNAMICALLY RESOLVE VARIABLES\n",
    "    \n",
    "    # B. Data Plane URL\n",
    "    try:\n",
    "        raw_dp_url_val = w.secrets.get_secret(scope=\"shscreds\", key=\"dpurl\").value\n",
    "        dp_url = safe_b64_decode(raw_dp_url_val).strip().rstrip('/')\n",
    "    except Exception:\n",
    "        return json.dumps({\"error\": \"Missing secret: shscreds/dpurl\"})\n",
    "\n",
    "    # C. Auth Cookies\n",
    "    raw_cookie = w.secrets.get_secret(scope=\"shscreds\", key=\"cookies\").value\n",
    "    cookies = {'DATAPLANE_DOMAIN_DBAUTH': safe_b64_decode(raw_cookie)}\n",
    "    \n",
    "    # 3. IDENTIFY APP\n",
    "    \n",
    "    c_info = w.clusters.get(cluster_id=cluster_id)\n",
    "    ctx_id = str(c_info.spark_context_id)\n",
    "    ctx_id = ctx_id if ctx_id.startswith(\"driver-\") else f\"driver-{ctx_id}\"\n",
    "    \n",
    "    api_base = f\"{dp_url}/sparkui/{cluster_id}/{ctx_id}/api/v1/applications\"\n",
    "    \n",
    "    # 4. FETCH DATA\n",
    "\n",
    "    appsrawtxt = requests.get(api_base, cookies=cookies).text\n",
    "    while appsrawtxt.find(\"Loading historical Spark UI\") > -1:\n",
    "      time.sleep(30)\n",
    "      appsrawtxt = requests.get(api_base, cookies=cookies).text\n",
    "\n",
    "    appsrawjson = json.loads(appsrawtxt)\n",
    "    app_id = appsrawjson[0][\"id\"]\n",
    "\n",
    "    sql_url = f\"{api_base}/{app_id}/sql\"\n",
    "    raw_sql_data = requests.get(sql_url, cookies=cookies).text\n",
    "\n",
    "    return raw_sql_data\n",
    "\n",
    "except Exception as e:\n",
    "    return json.dumps({\"error\": f\"UDF Error: {str(e)}\"})\n",
    "$$;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "LLMToolsDbx",
   "widgets": {
    "catalog_name": {
     "currentValue": "users",
     "nuid": "d41271e3-4402-49a8-b91f-a68e4888eb83",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "roberto_salcido",
     "nuid": "a004658c-5fd7-414c-98d6-7bc7c5d72e1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

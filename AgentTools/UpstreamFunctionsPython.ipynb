{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c141f4b7-643d-4037-a039-cbc9b7e4a191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or \"spark_observability\"\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e84fd2-671e-4928-88d5-4897d1f98fd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG IDENTIFIER(:catalog_name);\n",
    "USE SCHEMA IDENTIFIER(:schema_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f266605c-1393-4040-9438-da523a3dc1d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Service Credential Setup\n",
    "\n",
    "This notebook uses a **Databricks Service Credential** to authenticate with AWS APIs. The credential (`profiler-credential`) must be created before running the functions below.\n",
    "\n",
    "### Creating a Service Credential\n",
    "\n",
    "1. Navigate to **Catalog > External Data > Credentials** in your Databricks workspace\n",
    "2. Click **Create credential** and select **Service credential**\n",
    "3. Name it `profiler-credential` (or update the references in this notebook)\n",
    "4. Configure the IAM role ARN with the permissions listed below\n",
    "\n",
    "### Required IAM Permissions\n",
    "\n",
    "**EMR**\n",
    "- `elasticmapreduce:ListClusters`\n",
    "- `elasticmapreduce:DescribeCluster`\n",
    "- `elasticmapreduce:ListSteps`\n",
    "- `elasticmapreduce:DescribeStep`\n",
    "- `elasticmapreduce:CreatePersistentAppUI`\n",
    "- `elasticmapreduce:GetPersistentAppUIPresignedURL`\n",
    "- `elasticmapreduce:ListInstanceGroups`\n",
    "- `elasticmapreduce:ListInstanceFleets`\n",
    "\n",
    "**S3** (if writing output)\n",
    "- `s3:PutObject`\n",
    "- `s3:GetObject`\n",
    "- `s3:ListBucket`\n",
    "\n",
    "**STS**\n",
    "- `sts:GetCallerIdentity`\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Databricks: Create a Service Credential](https://docs.databricks.com/aws/en/connect/unity-catalog/cloud-services/service-credentials)\n",
    "- [AWS: EMR IAM Policies](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-iam-roles.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04a7161-6c6d-4ed8-9b1c-f1b75e471027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Grant access to yourself or your group\n",
    "GRANT ACCESS ON SERVICE CREDENTIAL `profiler-credential` TO `@domain.com`;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d660bcf-a4fb-4c2d-9709-6a033bf148a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server jobs for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshsjobsraw(emr_cluster_arn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data\n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        # Process each ARN in the current batch\n",
    "        for arn in batch:\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get jobs for each app\n",
    "                all_jobs = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        job_url = f\"{api_base}/applications/{app_id}/jobs\"\n",
    "                        job_response = req_session.get(job_url, allow_redirects=True, timeout=30)\n",
    "                        if job_response.status_code == 200:\n",
    "                            jobs = job_response.json()\n",
    "                            for job in jobs:\n",
    "                                job['app_id'] = app_id\n",
    "                                job['cluster_arn'] = arn\n",
    "                            all_jobs.extend(jobs)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"jobs\": all_jobs, \"count\": len(all_jobs)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb36d25b-29e8-4571-af5f-d5e996a36add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server stages for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshsstagesraw(emr_cluster_arn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data\n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        # Process each ARN in the current batch\n",
    "        for arn in batch:\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get stages for each app\n",
    "                all_stages = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        stage_url = f\"{api_base}/applications/{app_id}/stages\"\n",
    "                        stage_response = req_session.get(stage_url, allow_redirects=True, timeout=30)\n",
    "                        if stage_response.status_code == 200:\n",
    "                            stages = stage_response.json()\n",
    "                            for stage in stages:\n",
    "                                stage['app_id'] = app_id\n",
    "                                stage['cluster_arn'] = arn\n",
    "                            all_stages.extend(stages)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"stages\": all_stages, \"count\": len(all_stages)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71dce195-b3a0-4229-94cd-6b7518570fea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server SQL execution data for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshssqlraw(emr_cluster_arn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data\n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        # Process each ARN in the current batch\n",
    "        for arn in batch:\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get SQL queries for each app\n",
    "                all_sql = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        sql_url = f\"{api_base}/applications/{app_id}/sql\"\n",
    "                        sql_response = req_session.get(sql_url, allow_redirects=True, timeout=30)\n",
    "                        if sql_response.status_code == 200:\n",
    "                            sqlqueries = sql_response.json()\n",
    "                            for sql in sqlqueries:\n",
    "                                sql['app_id'] = app_id\n",
    "                                sql['cluster_arn'] = arn\n",
    "                            all_sql.extend(sqlqueries)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"sql\": all_sql, \"count\": len(all_sql)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "214f8a81-b051-4ab3-914b-3d76cf4a4a91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server executors for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshsexecutorsraw(emr_cluster_arn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data\n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        # Process each ARN in the current batch\n",
    "        for arn in batch:\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get executors for each app (using allexecutors endpoint)\n",
    "                all_executors = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        executor_url = f\"{api_base}/applications/{app_id}/allexecutors\"\n",
    "                        executor_response = req_session.get(executor_url, allow_redirects=True, timeout=30)\n",
    "                        if executor_response.status_code == 200:\n",
    "                            executors = executor_response.json()\n",
    "                            for executor in executors:\n",
    "                                executor['app_id'] = app_id\n",
    "                                executor['cluster_arn'] = arn\n",
    "                            all_executors.extend(executors)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"executors\": all_executors, \"count\": len(all_executors)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eeaf3245-d3f3-4d00-a6ce-7718ac7a409a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server environment info for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshsenvraw(emr_cluster_arn STRING)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data\n",
    "    for batch in iterator:\n",
    "        results = []\n",
    "        # Process each ARN in the current batch\n",
    "        for arn in batch:\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get environment info for each app\n",
    "                all_env = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        env_url = f\"{api_base}/applications/{app_id}/environment\"\n",
    "                        env_response = req_session.get(env_url, allow_redirects=True, timeout=30)\n",
    "                        if env_response.status_code == 200:\n",
    "                            env_data = env_response.json()\n",
    "                            env_data['app_id'] = app_id\n",
    "                            env_data['cluster_arn'] = arn\n",
    "                            all_env.append(env_data)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"environment\": all_env, \"count\": len(all_env)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d12195c-a7b3-4288-bb4f-0f3041220377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- List raw Spark History Server tasks for a given stage on an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_listshstasksraw(emr_cluster_arn STRING, stage_id INT)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "ENVIRONMENT (\n",
    "  dependencies = '[\"boto3==1.42.0\", \"requests\"]',\n",
    "  environment_version = \"None\"\n",
    ")\n",
    "-- update with your service credential name\n",
    "CREDENTIALS (\n",
    "  `profiler-credential` AS default_cred DEFAULT\n",
    ")\n",
    "PARAMETER STYLE PANDAS\n",
    "HANDLER 'handler_func'\n",
    "AS $$\n",
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import Iterator, Tuple\n",
    "from urllib.parse import urlparse\n",
    "from databricks.service_credentials import getServiceCredentialsProvider\n",
    "\n",
    "def handler_func(iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
    "    # Initialize credentials provider once\n",
    "    credential_variable = 'default_cred'\n",
    "    \n",
    "    provider = getServiceCredentialsProvider(credential_variable)\n",
    "    \n",
    "    # Cache boto3 clients by region to avoid recreating them for every row\n",
    "    client_cache = {}\n",
    "\n",
    "    def get_emr_client(region):\n",
    "        if region not in client_cache:\n",
    "            session = boto3.Session(botocore_session=provider, region_name=region)\n",
    "            client_cache[region] = session.client('emr', region_name=region)\n",
    "        return client_cache[region]\n",
    "\n",
    "    def wait_for_presigned_url_ready(emr_client, persistent_ui_id, max_wait=60, interval=5):\n",
    "        \"\"\"Poll until PresignedURLReady is True\"\"\"\n",
    "        total_waited = 0\n",
    "        while total_waited < max_wait:\n",
    "            response = emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=persistent_ui_id,\n",
    "                PersistentAppUIType='SHS'\n",
    "            )\n",
    "            if response.get(\"PresignedURLReady\", False):\n",
    "                return response.get(\"PresignedURL\")\n",
    "            time.sleep(interval)\n",
    "            total_waited += interval\n",
    "        return None\n",
    "\n",
    "    # Iterate through batches of data (tuple of Series for multi-param UDF)\n",
    "    for arn_batch, stage_id_batch in iterator:\n",
    "        results = []\n",
    "        # Process each row in the current batch\n",
    "        for arn, stage_id in zip(arn_batch, stage_id_batch):\n",
    "            try:\n",
    "                # Handle potential nulls or empty strings\n",
    "                if not arn:\n",
    "                    results.append(json.dumps({\"success\": False, \"error\": \"Empty ARN\", \"error_type\": \"ValueError\"}))\n",
    "                    continue\n",
    "\n",
    "                region = arn.split(\":\")[3]\n",
    "                emr_client = get_emr_client(region)\n",
    "\n",
    "                # Create persistent app UI\n",
    "                createapp = emr_client.create_persistent_app_ui(\n",
    "                    TargetResourceArn=arn\n",
    "                )\n",
    "                persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "                \n",
    "                # Wait for presigned URL to be ready, then get it\n",
    "                presigned_url = wait_for_presigned_url_ready(emr_client, persistent_ui_id)\n",
    "\n",
    "                if not presigned_url:\n",
    "                    results.append(json.dumps({\n",
    "                        \"success\": False, \n",
    "                        \"error\": \"Presigned URL not ready after waiting\", \n",
    "                        \"error_type\": \"TimeoutError\"\n",
    "                    }))\n",
    "                    continue\n",
    "                \n",
    "                # Parse the presigned URL to extract the base URL\n",
    "                # The presigned URL has auth params - we need base_url for API calls\n",
    "                parsed_url = urlparse(presigned_url)\n",
    "                base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/shs\"\n",
    "                api_base = f\"{base_url}/api/v1\"\n",
    "                \n",
    "                # Create a new session for this cluster and establish cookies\n",
    "                # by first visiting the presigned URL\n",
    "                req_session = requests.Session()\n",
    "                req_session.headers.update({\n",
    "                    \"User-Agent\": \"EMR-Observability-Client/1.0\",\n",
    "                    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\"\n",
    "                })\n",
    "                \n",
    "                # Visit presigned URL to establish authenticated session (sets cookies)\n",
    "                response = req_session.get(presigned_url, allow_redirects=True, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Now update headers for JSON API calls\n",
    "                req_session.headers.update({\"Accept\": \"application/json\"})\n",
    "                \n",
    "                # Fetch applications using the base URL (not the presigned URL)\n",
    "                app_url = f\"{api_base}/applications\"\n",
    "                app_response = req_session.get(app_url, allow_redirects=True, timeout=30)\n",
    "                app_response.raise_for_status()\n",
    "                apps = app_response.json()\n",
    "                \n",
    "                # Get tasks for the specified stage in each app\n",
    "                all_tasks = []\n",
    "                for app in apps:\n",
    "                    app_id = app.get('id')\n",
    "                    if app_id:\n",
    "                        # Use taskList endpoint for actual task data (stage_attempt=0)\n",
    "                        task_url = f\"{api_base}/applications/{app_id}/stages/{stage_id}/0/taskList\"\n",
    "                        task_response = req_session.get(task_url, allow_redirects=True, timeout=30)\n",
    "                        if task_response.status_code == 200:\n",
    "                            tasks = task_response.json()\n",
    "                            for task in tasks:\n",
    "                                task['app_id'] = app_id\n",
    "                                task['stage_id'] = stage_id\n",
    "                                task['cluster_arn'] = arn\n",
    "                            all_tasks.extend(tasks)\n",
    "                \n",
    "                results.append(json.dumps({\"success\": True, \"tasks\": all_tasks, \"count\": len(all_tasks)}))\n",
    "\n",
    "            except Exception as e:\n",
    "                # Return error JSON for this specific row instead of failing the whole batch\n",
    "                results.append(json.dumps({\"success\": False, \"error\": str(e), \"error_type\": type(e).__name__}))\n",
    "        \n",
    "        # Yield the results for this batch as a Series\n",
    "        yield pd.Series(results)\n",
    "$$;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4913a452-168e-4a20-bb4f-d60fba50aeda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get slowest Spark jobs for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_getslowestjobs(\n",
    "  clusterid STRING\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  jobId STRING, \n",
    "  name STRING, \n",
    "  description STRING, \n",
    "  submissionTime STRING, \n",
    "  completionTime STRING, \n",
    "  stageIds STRING, \n",
    "  status STRING, \n",
    "  numTasks DOUBLE, \n",
    "  numCompletedTasks DOUBLE, \n",
    "  numSkippedTasks DOUBLE, \n",
    "  numFailedTasks DOUBLE, \n",
    "  numCompletedStages DOUBLE, \n",
    "  numSkippedStages DOUBLE, \n",
    "  numFailedStages DOUBLE, \n",
    "  runtimesec LONG\n",
    ")\n",
    "COMMENT 'Calls SHS to get slowest jobs sorted by runtime'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshsjobsraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "jobs_extracted AS (\n",
    "  SELECT parsed_result:jobs::ARRAY<STRUCT<\n",
    "    jobId: STRING, \n",
    "    name: STRING, \n",
    "    description: STRING, \n",
    "    submissionTime: STRING, \n",
    "    completionTime: STRING, \n",
    "    stageIds: STRING, \n",
    "    status: STRING, \n",
    "    numTasks: DOUBLE, \n",
    "    numCompletedTasks: DOUBLE, \n",
    "    numSkippedTasks: DOUBLE, \n",
    "    numFailedTasks: DOUBLE, \n",
    "    numCompletedStages: DOUBLE, \n",
    "    numSkippedStages: DOUBLE, \n",
    "    numFailedStages: DOUBLE\n",
    "  >> AS jobmetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "exploded AS (\n",
    "  SELECT EXPLODE(jobmetrics) AS job\n",
    "  FROM jobs_extracted\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  job.jobId,\n",
    "  job.name,\n",
    "  job.description,\n",
    "  job.submissionTime,\n",
    "  job.completionTime,\n",
    "  job.stageIds,\n",
    "  job.status,\n",
    "  job.numTasks,\n",
    "  job.numCompletedTasks,\n",
    "  job.numSkippedTasks,\n",
    "  job.numFailedTasks,\n",
    "  job.numCompletedStages,\n",
    "  job.numSkippedStages,\n",
    "  job.numFailedStages,\n",
    "  TIMESTAMPDIFF(SECOND, TO_TIMESTAMP(job.submissionTime), TO_TIMESTAMP(job.completionTime)) AS runtimesec\n",
    "FROM exploded \n",
    "ORDER BY runtimesec DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bd9e331-3a16-46cf-9b61-ad5a8f48b358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get slowest Spark stages for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_getsloweststages(\n",
    "  clusterid STRING\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  stageId STRING, \n",
    "  attemptId STRING, \n",
    "  name STRING, \n",
    "  submissionTime STRING, \n",
    "  completionTime STRING, \n",
    "  status STRING, \n",
    "  numTasks DOUBLE, \n",
    "  numCompletedTasks DOUBLE, \n",
    "  numSkippedTasks DOUBLE, \n",
    "  numFailedTasks DOUBLE,\n",
    "  memoryBytesSpilled LONG, \n",
    "  diskBytesSpilled LONG, \n",
    "  inputBytes LONG, \n",
    "  inputRecords LONG, \n",
    "  outputBytes LONG, \n",
    "  outputRecords LONG, \n",
    "  shuffleReadBytes LONG, \n",
    "  shuffleReadRecords LONG, \n",
    "  shuffleWriteBytes LONG, \n",
    "  shuffleWriteRecords LONG, \n",
    "  runtimesec LONG\n",
    ")\n",
    "COMMENT 'Calls SHS to get slowest stages sorted by runtime'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshsstagesraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "stages_extracted AS (\n",
    "  SELECT parsed_result:stages::ARRAY<STRUCT<\n",
    "    stageId: STRING, \n",
    "    attemptId: STRING, \n",
    "    name: STRING, \n",
    "    submissionTime: STRING, \n",
    "    completionTime: STRING, \n",
    "    status: STRING, \n",
    "    numTasks: DOUBLE, \n",
    "    numCompletedTasks: DOUBLE, \n",
    "    numSkippedTasks: DOUBLE, \n",
    "    numFailedTasks: DOUBLE,\n",
    "    memoryBytesSpilled: LONG, \n",
    "    diskBytesSpilled: LONG, \n",
    "    inputBytes: LONG, \n",
    "    inputRecords: LONG, \n",
    "    outputBytes: LONG, \n",
    "    outputRecords: LONG, \n",
    "    shuffleReadBytes: LONG, \n",
    "    shuffleReadRecords: LONG, \n",
    "    shuffleWriteBytes: LONG, \n",
    "    shuffleWriteRecords: LONG\n",
    "  >> AS stagemetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "exploded AS (\n",
    "  SELECT EXPLODE(stagemetrics) AS stage\n",
    "  FROM stages_extracted\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  stage.stageId,\n",
    "  stage.attemptId,\n",
    "  stage.name,\n",
    "  stage.submissionTime,\n",
    "  stage.completionTime,\n",
    "  stage.status,\n",
    "  stage.numTasks,\n",
    "  stage.numCompletedTasks,\n",
    "  stage.numSkippedTasks,\n",
    "  stage.numFailedTasks,\n",
    "  stage.memoryBytesSpilled,\n",
    "  stage.diskBytesSpilled,\n",
    "  stage.inputBytes,\n",
    "  stage.inputRecords,\n",
    "  stage.outputBytes,\n",
    "  stage.outputRecords,\n",
    "  stage.shuffleReadBytes,\n",
    "  stage.shuffleReadRecords,\n",
    "  stage.shuffleWriteBytes,\n",
    "  stage.shuffleWriteRecords,\n",
    "  TIMESTAMPDIFF(SECOND, TO_TIMESTAMP(stage.submissionTime), TO_TIMESTAMP(stage.completionTime)) AS runtimesec\n",
    "FROM exploded \n",
    "ORDER BY runtimesec DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb0c63a-f18c-4e34-979f-d993508b107a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get slowest SQL executions for an EMR cluster\n",
    "CREATE OR REPLACE FUNCTION emr_getslowestsql(\n",
    "  clusterid STRING\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  id LONG, \n",
    "  status STRING, \n",
    "  description STRING, \n",
    "  planDescription STRING, \n",
    "  submissionTime STRING, \n",
    "  duration LONG, \n",
    "  successJobIds STRING, \n",
    "  failedJobIds STRING, \n",
    "  nodes ARRAY<STRUCT<nodeId: INT, nodeName: STRING, metrics: ARRAY<STRUCT<name: STRING, value: STRING>>>>\n",
    ")\n",
    "COMMENT 'Calls SHS to get slowest SQL queries sorted by duration'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshssqlraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "sql_extracted AS (\n",
    "  SELECT parsed_result:sql::ARRAY<STRUCT<\n",
    "    id: LONG, \n",
    "    status: STRING, \n",
    "    description: STRING, \n",
    "    planDescription: STRING, \n",
    "    submissionTime: STRING, \n",
    "    duration: LONG, \n",
    "    successJobIds: STRING, \n",
    "    failedJobIds: STRING, \n",
    "    nodes: ARRAY<STRUCT<nodeId: INT, nodeName: STRING, metrics: ARRAY<STRUCT<name: STRING, value: STRING>>>>\n",
    "  >> AS sqlmetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "exploded AS (\n",
    "  SELECT EXPLODE(sqlmetrics) AS sql_query\n",
    "  FROM sql_extracted\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  sql_query.id,\n",
    "  sql_query.status,\n",
    "  sql_query.description,\n",
    "  sql_query.planDescription,\n",
    "  sql_query.submissionTime,\n",
    "  sql_query.duration,\n",
    "  sql_query.successJobIds,\n",
    "  sql_query.failedJobIds,\n",
    "  sql_query.nodes\n",
    "FROM exploded \n",
    "ORDER BY sql_query.duration DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d1b24cd-142d-4bb8-97b7-902b5cc53d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get a specific stage by ID\n",
    "CREATE OR REPLACE FUNCTION emr_getstage(\n",
    "  clusterid STRING, \n",
    "  stageid INT\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  stageId STRING, \n",
    "  attemptId STRING, \n",
    "  name STRING, \n",
    "  submissionTime STRING, \n",
    "  completionTime STRING, \n",
    "  status STRING, \n",
    "  numTasks DOUBLE, \n",
    "  numCompletedTasks DOUBLE, \n",
    "  numSkippedTasks DOUBLE, \n",
    "  numFailedTasks DOUBLE,\n",
    "  memoryBytesSpilled LONG, \n",
    "  diskBytesSpilled LONG, \n",
    "  inputBytes LONG, \n",
    "  inputRecords LONG, \n",
    "  outputBytes LONG, \n",
    "  outputRecords LONG, \n",
    "  shuffleReadBytes LONG, \n",
    "  shuffleReadRecords LONG, \n",
    "  shuffleWriteBytes LONG, \n",
    "  shuffleWriteRecords LONG, \n",
    "  runtimesec LONG\n",
    ")\n",
    "COMMENT 'Calls SHS to get a specific stage by ID'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshsstagesraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "stages_extracted AS (\n",
    "  SELECT parsed_result:stages::ARRAY<STRUCT<\n",
    "    stageId: STRING, \n",
    "    attemptId: STRING, \n",
    "    name: STRING, \n",
    "    submissionTime: STRING, \n",
    "    completionTime: STRING, \n",
    "    status: STRING, \n",
    "    numTasks: DOUBLE, \n",
    "    numCompletedTasks: DOUBLE, \n",
    "    numSkippedTasks: DOUBLE, \n",
    "    numFailedTasks: DOUBLE,\n",
    "    memoryBytesSpilled: LONG, \n",
    "    diskBytesSpilled: LONG, \n",
    "    inputBytes: LONG, \n",
    "    inputRecords: LONG, \n",
    "    outputBytes: LONG, \n",
    "    outputRecords: LONG, \n",
    "    shuffleReadBytes: LONG, \n",
    "    shuffleReadRecords: LONG, \n",
    "    shuffleWriteBytes: LONG, \n",
    "    shuffleWriteRecords: LONG\n",
    "  >> AS stagemetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "exploded AS (\n",
    "  SELECT EXPLODE(stagemetrics) AS stage\n",
    "  FROM stages_extracted\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  stage.stageId,\n",
    "  stage.attemptId,\n",
    "  stage.name,\n",
    "  stage.submissionTime,\n",
    "  stage.completionTime,\n",
    "  stage.status,\n",
    "  stage.numTasks,\n",
    "  stage.numCompletedTasks,\n",
    "  stage.numSkippedTasks,\n",
    "  stage.numFailedTasks,\n",
    "  stage.memoryBytesSpilled,\n",
    "  stage.diskBytesSpilled,\n",
    "  stage.inputBytes,\n",
    "  stage.inputRecords,\n",
    "  stage.outputBytes,\n",
    "  stage.outputRecords,\n",
    "  stage.shuffleReadBytes,\n",
    "  stage.shuffleReadRecords,\n",
    "  stage.shuffleWriteBytes,\n",
    "  stage.shuffleWriteRecords,\n",
    "  TIMESTAMPDIFF(SECOND, TO_TIMESTAMP(stage.submissionTime), TO_TIMESTAMP(stage.completionTime)) AS runtimesec\n",
    "FROM exploded \n",
    "WHERE stage.stageId = stageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ced878-5672-4d5e-8383-c01c06047753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Get a specific executor by ID\n",
    "CREATE OR REPLACE FUNCTION emr_getexecutor(\n",
    "  clusterid STRING, \n",
    "  executorid STRING\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  id STRING, \n",
    "  memoryUsed DOUBLE, \n",
    "  diskUsed DOUBLE, \n",
    "  totalCores DOUBLE, \n",
    "  addTime STRING, \n",
    "  removeTime STRING, \n",
    "  maxTasks DOUBLE, \n",
    "  completedTasks DOUBLE, \n",
    "  totalTasks DOUBLE, \n",
    "  totalDuration DOUBLE, \n",
    "  totalGCTime DOUBLE, \n",
    "  totalInputBytes LONG, \n",
    "  totalShuffleRead LONG, \n",
    "  totalShuffleWrite LONG, \n",
    "  maxMemory LONG, \n",
    "  uptime LONG\n",
    ")\n",
    "COMMENT 'Calls SHS to get a specific executor by ID'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshsexecutorsraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "executors_extracted AS (\n",
    "  SELECT parsed_result:executors::ARRAY<STRUCT<\n",
    "    id: STRING, \n",
    "    memoryUsed: DOUBLE, \n",
    "    diskUsed: DOUBLE, \n",
    "    totalCores: DOUBLE, \n",
    "    addTime: STRING, \n",
    "    removeTime: STRING, \n",
    "    maxTasks: DOUBLE, \n",
    "    completedTasks: DOUBLE, \n",
    "    totalTasks: DOUBLE, \n",
    "    totalDuration: DOUBLE, \n",
    "    totalGCTime: DOUBLE, \n",
    "    totalInputBytes: LONG, \n",
    "    totalShuffleRead: LONG, \n",
    "    totalShuffleWrite: LONG, \n",
    "    maxMemory: LONG\n",
    "  >> AS execmetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "exploded AS (\n",
    "  SELECT EXPLODE(execmetrics) AS executor\n",
    "  FROM executors_extracted\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  executor.id,\n",
    "  executor.memoryUsed,\n",
    "  executor.diskUsed,\n",
    "  executor.totalCores,\n",
    "  executor.addTime,\n",
    "  executor.removeTime,\n",
    "  executor.maxTasks,\n",
    "  executor.completedTasks,\n",
    "  executor.totalTasks,\n",
    "  executor.totalDuration,\n",
    "  executor.totalGCTime,\n",
    "  executor.totalInputBytes,\n",
    "  executor.totalShuffleRead,\n",
    "  executor.totalShuffleWrite,\n",
    "  executor.maxMemory,\n",
    "  TIMESTAMPDIFF(SECOND, TO_TIMESTAMP(executor.addTime), TO_TIMESTAMP(executor.removeTime)) AS uptime\n",
    "FROM exploded \n",
    "WHERE executor.id = executorid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "075bd4f5-ff2d-47eb-9038-c3932d78952b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Analyze Photon compatibility of SQL queries\n",
    "CREATE OR REPLACE FUNCTION emr_photonanalysis(\n",
    "  clusterid STRING\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  sql_id LONG,\n",
    "  cluster_arn STRING,\n",
    "  photon_compatible_nodes INT,\n",
    "  total_nodes INT,\n",
    "  photon_percentage DOUBLE\n",
    ")\n",
    "COMMENT 'Analyzes EMR Spark History Server SQL metrics to estimate how much of a Spark job would benefit from Photon'\n",
    "RETURN\n",
    "\n",
    "WITH raw AS (\n",
    "  SELECT try_parse_json(emr_listshssqlraw(clusterid)) AS parsed_result\n",
    "),\n",
    "\n",
    "sql_extracted AS (\n",
    "  SELECT parsed_result:sql::ARRAY<STRUCT<\n",
    "    id: LONG, \n",
    "    status: STRING, \n",
    "    description: STRING, \n",
    "    planDescription: STRING, \n",
    "    submissionTime: STRING, \n",
    "    duration: LONG, \n",
    "    successJobIds: STRING, \n",
    "    failedJobIds: STRING,\n",
    "    app_id: STRING,\n",
    "    cluster_arn: STRING,\n",
    "    nodes: ARRAY<STRUCT<nodeId: INT, nodeName: STRING, metrics: ARRAY<STRUCT<name: STRING, value: STRING>>>>\n",
    "  >> AS sqlmetrics\n",
    "  FROM raw\n",
    "  WHERE parsed_result:success::BOOLEAN = true\n",
    "),\n",
    "\n",
    "sql_exploded AS (\n",
    "  SELECT EXPLODE(sqlmetrics) AS sql_query\n",
    "  FROM sql_extracted\n",
    "),\n",
    "\n",
    "nodes_exploded AS (\n",
    "  SELECT \n",
    "    sql_query.id AS sql_id,\n",
    "    sql_query.cluster_arn,\n",
    "    node\n",
    "  FROM sql_exploded\n",
    "  LATERAL VIEW EXPLODE(sql_query.nodes) AS node\n",
    "),\n",
    "\n",
    "photon_check AS (\n",
    "  SELECT \n",
    "    sql_id,\n",
    "    cluster_arn,\n",
    "    node.nodeName,\n",
    "    CASE \n",
    "      WHEN node.nodeName LIKE '%MapElements%' THEN 0 \n",
    "      WHEN node.nodeName LIKE '%MapPartitions%' THEN 0 \n",
    "      WHEN node.nodeName LIKE '%Scan csv%' THEN 0\n",
    "      WHEN node.nodeName LIKE '%Scan json%' THEN 0 \n",
    "      WHEN node.nodeName LIKE '%PythonUDF%' THEN 0 \n",
    "      WHEN node.nodeName LIKE '%ScalaUDF%' THEN 0 \n",
    "      WHEN node.nodeName LIKE '%FlatMapGroupsInPandas%' THEN 0  \n",
    "      WHEN node.nodeName LIKE '%DeserializeToObject%' THEN 0\n",
    "      WHEN node.nodeName LIKE '%SerializeFromObject%' THEN 0  \n",
    "      ELSE 1 \n",
    "    END AS photon_compatible\n",
    "  FROM nodes_exploded\n",
    "),\n",
    "\n",
    "aggregated AS (\n",
    "  SELECT \n",
    "    sql_id,\n",
    "    cluster_arn,\n",
    "    SUM(photon_compatible) AS photon_compatible_nodes,\n",
    "    COUNT(*) AS total_nodes,\n",
    "    TRY_DIVIDE(SUM(photon_compatible), COUNT(*)) AS photon_percentage\n",
    "  FROM photon_check \n",
    "  GROUP BY sql_id, cluster_arn\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  sql_id,\n",
    "  cluster_arn,\n",
    "  photon_compatible_nodes,\n",
    "  total_nodes,\n",
    "  photon_percentage\n",
    "FROM aggregated\n",
    "ORDER BY photon_percentage DESC"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7097269370522660,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "UpstreamFunctionsPython",
   "widgets": {
    "catalog_name": {
     "currentValue": "profiler",
     "nuid": "4a92145b-48fb-41b9-93fc-620d12a98d32",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "emroutput",
     "nuid": "6f236fd2-77d5-4666-b788-5e0399bc3495",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

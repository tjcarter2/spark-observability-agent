{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f58221c2-26b8-4c36-9ee4-a3c1180fa0f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ineff jobs across all spark jobs, agged data\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.ineffjobs()\n",
    "RETURNS TABLE (cluster_id string, ineffstageagg double)\n",
    "COMMENT 'Returns aggregated data regarding inefficient spark jobs'\n",
    "RETURN SELECT cluster_id, ineffstageagg\n",
    "  FROM prodrs.spark_observability.ineffjobagg;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc704456-3240-4726-8585-7b1b6943f0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- ineff jobs across all spark jobs, raw unagged data\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.ineffjobsraw()\n",
    "RETURNS TABLE (cluster_id string, stageid int, skewbinary int, spillbinary int, diskspillbinary int, chunkbinary int, prbinary int, task_completion_rate double)\n",
    "COMMENT 'Returns raw data regarding inefficient spark jobs'\n",
    "RETURN SELECT cluster_id, stage_id, skewbinary, spillbinary, diskspillbinary, chunkbinary, prbinary, task_completion_rate\n",
    "  FROM prodrs.spark_observability.ineffjobraw\n",
    "Where goldcheckfilter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b432c0a-8e89-4ece-ad4a-0a08069d131f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- photon analysis for EMR \n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.emrphotonanalysis()\n",
    "RETURNS TABLE\n",
    "COMMENT 'Returns analysis regarding which spark jobs are most likely to benefit from photon'\n",
    "RETURN SELECT cluster_name, jobphotonperc\n",
    "  FROM prodrs.spark_observability.photonanalysis\n",
    "  where jobphotonperc > .8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7b29d7-cb5b-40a9-8933-b243bb022d04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getsparkcontext(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls to db api to get spark context'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'clusterapi',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"api/2.1/clusters/get?cluster_id=%s\", clusterid))\n",
    "    )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "105b06c7-6024-4227-98cd-315b77387e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listappsraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to list applications submitted against spark cluster'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788d56aa-f380-4a46-8b33-36b5c7f06520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getappid(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get appid'\n",
    "RETURN try_parse_json(prodrs.spark_observability.listappsraw(clusterid))::array<struct<id:string>>[0][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad728d1-557f-4cd4-b909-3e02cf2e1422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshsjobsraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get jobs list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/jobs\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid)),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc24c31-fa04-4716-89c1-e1fc59f964fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshsstagesraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get stages list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/stages\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid)),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f984d94d-3308-4454-a067-2a5cf9cc9999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshssqlraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get sql list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/sql\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid)),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cb1e5d-bf68-486d-b4b7-c58ce6012d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshsexecutorsraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get executors list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/allexecutors\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid)),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f542aa-494d-4e21-810b-2ef6712801a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshsenvraw(\n",
    "  clusterid string\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get environment raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/environment\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid)),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5097d68-99e3-4499-ba72-1341c89a1ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.listshstasksraw(\n",
    "  clusterid string, stageid int\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get tasks list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/stages/%s/0/taskSummary\", clusterid, prodrs.spark_observability.getsparkcontext(clusterid):spark_context_id, prodrs.spark_observability.getappid(clusterid), stageid),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c814473-efe4-426b-9561-4a3992399ed3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766855302483}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--list jobs mcp tool\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getslowestjobs(\n",
    "  clusterid string\n",
    ")\n",
    "\n",
    "RETURNS TABLE (jobId string, name string, description string, submissionTime string, completionTime string, stageIds string, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, runtimesec long)\n",
    "COMMENT 'Calls shs to get slowestjobs'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(prodrs.spark_observability.listshsjobsraw(clusterid))::array<struct<jobId:string, name:string, description:string, submissionTime:string, completionTime:string, stageIds:string, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double>> as jobmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(jobmetrics) as jobmetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select jobmetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(jobmetricsexp.submissionTime), to_timestamp(jobmetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "order by runtimesec desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30398d7-e3a2-422c-aa32-2d4ad660c7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getsloweststages(\n",
    "  clusterid string\n",
    ")\n",
    "\n",
    "RETURNS TABLE (stageId string, attemptId string, name string, description string, submissionTime string, completionTime string, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, memoryBytesSpilled long, diskBytesSpilled long, inputBytes long, inputRecords long, outputBytes long, outputRecords long, shuffleReadBytes long, shuffleReadRecords long, shuffleWriteBytes long, shuffleWriteRecords long, runtimesec long)\n",
    "COMMENT 'Calls shs to get slowest stages'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(prodrs.spark_observability.listshsstagesraw(clusterid))::array<struct<stageId:string, attemptId:string, name:string, description:string, submissionTime:string, completionTime:string, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double, memoryBytesSpilled:long, diskBytesSpilled:long, inputBytes:long, inputRecords:long, outputBytes:long, outputRecords:long, shuffleReadBytes:long, shuffleReadRecords:long, shuffleWriteBytes:long, shuffleWriteRecords:long >> as stagemetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(stagemetrics) as stagemetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select stagemetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(stagemetricsexp.submissionTime), to_timestamp(stagemetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "order by runtimesec desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1815a1-e592-40c5-8b46-89509c7bdc11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--list jobs mcp tool\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getslowestsql(\n",
    "  clusterid string\n",
    ")\n",
    "\n",
    "RETURNS TABLE (id long, status string, description string, planDescription string, submissionTime string, duration long, successJobIds string, failedJobIds string, nodes array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>>)\n",
    "COMMENT 'Calls shs to get slowest sql queries'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(prodrs.spark_observability.listshssqlraw(clusterid))::array<struct<id:long, status:string, description:string, planDescription:string, submissionTime:string, duration:long, successJobIds:string, failedJobIds:string, nodes: array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>> >> as sqlmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(sqlmetrics) as sqlmetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select sqlmetricsexp.*\n",
    "from explode \n",
    "order by sqlmetricsexp.duration desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6677d367-9ac6-4f0a-a95a-e0616119799f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--list jobs mcp tool\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getstage(\n",
    "  clusterid string, stageid int\n",
    ")\n",
    "\n",
    "RETURNS TABLE (stageId string, attemptId string, name string, description string, submissionTime string, completionTime string, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, memoryBytesSpilled long, diskBytesSpilled long, inputBytes long, inputRecords long, outputBytes long, outputRecords long, shuffleReadBytes long, shuffleReadRecords long, shuffleWriteBytes long, shuffleWriteRecords long, runtimesec long)\n",
    "COMMENT 'Calls shs to get a specific stage'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(prodrs.spark_observability.listshsstagesraw(clusterid))::array<struct<stageId:string, attemptId:string, name:string, description:string, submissionTime:string, completionTime:string, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double, memoryBytesSpilled:long, diskBytesSpilled:long, inputBytes:long, inputRecords:long, outputBytes:long, outputRecords:long, shuffleReadBytes:long, shuffleReadRecords:long, shuffleWriteBytes:long, shuffleWriteRecords:long >> as stagemetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(stagemetrics) as stagemetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select stagemetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(stagemetricsexp.submissionTime), to_timestamp(stagemetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "where stagemetricsexp.stageId = stageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d16ba4d-3c3a-4b67-8545-7b1c11578106",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--list jobs mcp tool\n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.getexecutor(\n",
    "  clusterid string, executorid int\n",
    ")\n",
    "\n",
    "RETURNS TABLE (id string, memoryUsed double, diskUsed double, totalCores double, addTime string, removeTime string, maxTasks double, completedTasks double, totalTasks double, totalDuration double, totalGCTime double, totalInputBytes long, totalShuffleRead long, totalShuffleWrite long, maxMemory long, uptime long)\n",
    "COMMENT 'Calls shs to get a specific executor'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(prodrs.spark_observability.listshsexecutorsraw(clusterid))::array<struct<id:string, memoryUsed:double, diskUsed:double, totalCores:double, addTime:string, removeTime:string, maxTasks:double, completedTasks:double, totalTasks:double, totalDuration:double, totalGCTime:double, totalInputBytes:long, totalShuffleRead:long, totalShuffleWrite:long, maxMemory:long >> as execmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(execmetrics) as execmetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select execmetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(execmetricsexp.addTime), to_timestamp(execmetricsexp.removeTime)) as uptime\n",
    "from explode \n",
    "where execmetricsexp.id = executorid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4281939b-92bc-44a2-b1a7-03ee1a0aa029",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- listjobs for single EMR spark job \n",
    "CREATE OR REPLACE FUNCTION prodrs.spark_observability.list_jobs_emr(emr_cluster_arn STRING)\n",
    "RETURNS ARRAY\n",
    "LANGUAGE PYTHON\n",
    "AS $$\n",
    "\n",
    "emr_cluster_arn = {emr_cluster_arn}\n",
    "region = {emr_cluster_arn}split(\":\")[3]  # Extract region from ARN # Initialize boto3 client emr_client = boto3.client( \"emr\", region_name=region, )\n",
    "\n",
    "createapp = emr_client.create_persistent_app_ui(\n",
    "                TargetResourceArn=emr_cluster_arn\n",
    "            )\n",
    "persistent_ui_id = createapp.get(\"PersistentAppUIId\")\n",
    "\n",
    "genurl = self.emr_client.get_persistent_app_ui_presigned_url(\n",
    "                PersistentAppUIId=self.persistent_ui_id, PersistentAppUIType=ui_type\n",
    "            )\n",
    "presigned_url = genurl.get(\"PresignedURL\")\n",
    "Base_url = presigned_url \n",
    "session = requests.Session() \n",
    "session.headers.update(\n",
    "            {\n",
    "                \"User-Agent\": \"EMR-Persistent-UI-Client/1.0\",\n",
    "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "                \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "                \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                \"Connection\": \"keep-alive\",\n",
    "                \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            }\n",
    "        )\n",
    "response = session.get(presigned_url, allow_redirects=True) \n",
    "Cookies = session.cookies\n",
    "app_url = base_url + api/v1/applications\n",
    "response = session.get(app_url, cookies=cookies, allow_redirects=True) \n",
    "Responsejson = response.json()\n",
    "appid = Responsejson.get(‘id’)\n",
    "job_url = app_url + appid + ‘jobs’\n",
    "jobresponse = session.get(job_url, cookies=cookies, allow_redirects=True) \n",
    "Jobresponsejson = jobsresponse.json()\n",
    "return Jobresponsejson"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7680324554633555,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "agentddlprod",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88aee1a8-c986-42cb-b6c8-5bf76fc6255b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or \"spark_observability\"\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fdafbfe-9ae3-4448-9021-816b42b2599f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG IDENTIFIER(:catalog_name);\n",
    "USE SCHEMA IDENTIFIER(:schema_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9da65e6c-f9a0-434a-b8f3-d968f026ed65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION emr_sqlmetrics(\n",
    "  clusterid string, limit double default 20000, ranking double DEFAULT 0 \n",
    ")\n",
    "\n",
    "RETURNS TABLE (nodestring STRING, successJobIds array<string>, stringlength string, rank double)\n",
    "COMMENT 'this tool returns string snippet of node metrics for a spark sql query, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(emr_listshssqlraw(clusterid))::array<struct<id:long, status:string, description:string, planDescription:string, submissionTime:string, duration:long, successJobIds:array<string>, failedJobIds:string, nodes: array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>> >> as sqlmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(sqlmetrics) as sqlmetricsexp\n",
    "  from raw\n",
    "),\n",
    "\n",
    "pu as (select to_json(sqlmetricsexp.nodes) as nodestring, sqlmetricsexp.successJobIds as jobids,\n",
    "len(to_binary(to_json(sqlmetricsexp.nodes), \"UTF-8\")) as stringlength,\n",
    "rank() over (order by len(to_binary(to_json(sqlmetricsexp.nodes), \"UTF-8\")) desc) as rank\n",
    "from explode)\n",
    "\n",
    "select nodestring, jobids, stringlength, rank\n",
    "from pu \n",
    "where stringlength < limit\n",
    "and if(ranking = 0, 1 = 1, rank = ranking)\n",
    "order by stringlength desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d39b2a9-3429-450b-a7b4-fea0cb7c5058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION emr_jobmetrics(\n",
    "  clusterid string, jobidsarr array<string>\n",
    ")\n",
    "\n",
    "RETURNS TABLE (jobId string, name string, description string, submissionTime string, completionTime string, stageIds array<string>, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, runtimesec long)\n",
    "COMMENT 'this tool returns spark history server job metrics for a spark applications, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(emr_listshsjobsraw(clusterid))::array<struct<jobId:string, name:string, description:string, submissionTime:string, completionTime:string, stageIds:array<string>, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double>> as jobmetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(jobmetrics) as jobmetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select jobmetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(jobmetricsexp.submissionTime), to_timestamp(jobmetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "where array_contains(jobidsarr, jobmetricsexp.jobid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f71b207-4dc0-481e-acd1-31545536b137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION emr_stagemetrics(\n",
    "  clusterid string, stageidsarr array<string>\n",
    ")\n",
    "\n",
    "RETURNS TABLE (stageId string, attemptId string, name string, description string, submissionTime string, completionTime string, status string, numTasks double, numCompletedTasks double, numSkippedTasks double, numFailedTasks double, numCompletedStages double, numSkippedStages double, numFailedStages double, memoryBytesSpilled long, diskBytesSpilled long, inputBytes long, inputRecords long, outputBytes long, outputRecords long, shuffleReadBytes long, shuffleReadRecords long, shuffleWriteBytes long, shuffleWriteRecords long, runtimesec long)\n",
    "COMMENT 'this tool returns spark history server stage metrics for a spark applications, useful for tuning recommendations'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(emr_listshsstagesraw(clusterid))::array<struct<stageId:string, attemptId:string, name:string, description:string, submissionTime:string, completionTime:string, status:string, numTasks:double, numCompletedTasks:double, numSkippedTasks:double, numFailedTasks:double, numCompletedStages:double, numSkippedStages:double, numFailedStages:double, memoryBytesSpilled:long, diskBytesSpilled:long, inputBytes:long, inputRecords:long, outputBytes:long, outputRecords:long, shuffleReadBytes:long, shuffleReadRecords:long, shuffleWriteBytes:long, shuffleWriteRecords:long >> as stagemetrics),\n",
    "\n",
    "explode as (\n",
    "  select explode(stagemetrics) as stagemetricsexp\n",
    "  from raw\n",
    ")\n",
    "\n",
    "select stagemetricsexp.*,\n",
    "timestampdiff(second, to_timestamp(stagemetricsexp.submissionTime), to_timestamp(stagemetricsexp.completionTime)) as runtimesec\n",
    "from explode \n",
    "where array_contains(stageidsarr, stagemetricsexp.stageId) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73754aae-0499-49f7-8be8-f9dde665be22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION emr_taskmetrics(\n",
    "  clusterid string, stageid int\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Calls shs to get tasks list raw'\n",
    "RETURN (\n",
    "  http_request(\n",
    "  conn => 'shsjobs',\n",
    "  method => 'GET',\n",
    "  path => format_string(\"sparkui/%s/driver-%s/api/v1/applications/%s/stages/%s/0/taskSummary\", clusterid, getsparkcontext(clusterid):spark_context_id, getappid(clusterid), stageid),\n",
    "   headers => map(\n",
    "       'Cookie', format_string(\"DATAPLANE_DOMAIN_DBAUTH=%s\", secret(\"shscreds\", \"cookies\")))\n",
    "    )\n",
    "  )\n",
    ".text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe7c81b0-4a51-4a0a-8b10-85a7ad861187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Approach 2: Define photon logic upfront, execute within the UDF, have LLM return final value. This is only meant to be directionally accurate. SHS does not expose expressions. Further this is not a weighted average as it treats all operators essentially the same. \n",
    "\n",
    "CREATE OR REPLACE FUNCTION emr_photonmetrics(\n",
    "  clusterid string\n",
    ")\n",
    "\n",
    "RETURNS DOUBLE\n",
    "COMMENT 'Analyzes spark history server sql metrics to derive estimate of how much of spark job would benefit from photon'\n",
    "RETURN\n",
    "\n",
    "with raw as (\n",
    "  select try_parse_json(emr_listshssqlraw(clusterid))::array<struct<id:long, status:string, description:string, planDescription:string, submissionTime:string, duration:long, successJobIds:string, failedJobIds:string, nodes: array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>> >> as sqlmetrics),\n",
    "\n",
    "firstexplode as (\n",
    "  select explode(sqlmetrics) as sqlmetricsexp\n",
    "  from raw\n",
    "),\n",
    "\n",
    "secexplode as (\n",
    "  select sqlmetricsexp.*, nodemetrics\n",
    "  from firstexplode\n",
    "  lateral view explode(sqlmetricsexp.nodes) as nodemetrics\n",
    "),\n",
    "\n",
    "photoncheck as (select *, case when nodemetrics.nodeName = 'MapElements' then 0 \n",
    "when nodemetrics.nodeName = 'MapPartitions' then 0 \n",
    "when nodemetrics.nodeName = 'Scan csv' then 0\n",
    "when nodemetrics.nodeName = 'Scan json' then 0 \n",
    "when nodemetrics.nodeName = 'PythonUDF' then 0 \n",
    "when nodemetrics.nodeName = 'ScalaUDF' then 0 \n",
    "when nodemetrics.nodeName = 'FlatMapGroupsInPandas' then 0  \n",
    "when nodemetrics.nodeName = 'DeserializeToObject' then 0\n",
    "when nodemetrics.nodeName = 'SerializeFromObject' then 0  \n",
    "else 1 end as photonbinary\n",
    "from secexplode),\n",
    "\n",
    "jobcheck as (select try_divide(sum(photonbinary), count(*)) as jobphotonperc \n",
    "from photoncheck \n",
    "group by all)\n",
    "\n",
    "select jobphotonperc\n",
    "from jobcheck"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6482493832272697,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "LLMToolsEmr",
   "widgets": {
    "catalog_name": {
     "currentValue": "prodrs",
     "nuid": "d41271e3-4402-49a8-b91f-a68e4888eb83",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "spark_observability",
     "nuid": "a004658c-5fd7-414c-98d6-7bc7c5d72e1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

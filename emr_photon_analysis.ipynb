{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a60d1bb-706e-49e6-8f2b-f735826597c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Photon Compatibility Analysis\n",
    "This cell analyzes the `sql_df` DataFrame to identify how much of a Spark application's workload can be accelerated by Photon.\n",
    "It parses the JSON query plans, flags compatible operations, and calculates a \"Photon Compatibility Score\" for each application.\n",
    "A higher score indicates a greater portion of the application's SQL workload is Photon-compatible, suggesting better performance. A low score highlights opportunities for optimization by refactoring non-compatible operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c109a859-5d9e-45d1-bd0f-37b39c43c070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or \"spark_observability\"\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62757088-041d-4ef6-8f07-cd8f08df3771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, explode, when, count, avg\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# A list of common Spark plan operations that are NOT accelerated by Photon.\n",
    "# This list can be extended as needed based on specific workloads.\n",
    "NON_PHOTON_OPERATIONS = [\n",
    "    \"MapElements\",       # Often associated with Scala/Java UDFs\n",
    "    \"MapPartitions\",     # Can be used by various custom operations\n",
    "    \"PythonUDF\",         # Python User-Defined Functions\n",
    "    \"ScalaUDF\",          # Scala User-Defined Functions\n",
    "    \"FlatMapGroupsInPandas\", # Pandas UDFs\n",
    "    \"Scan csv\",          # Photon does not accelerate CSV scans\n",
    "    \"Scan json\",          # Photon does not accelerate JSON scans\n",
    "    \"FlatMapGroupsInPandas\",\n",
    "    \"DeserializeToObject\",\n",
    "    \"SerializeFromObject\"\n",
    "]\n",
    "\n",
    "# Define the schema to extract the nodeName from the JSON query plan.\n",
    "# We only need the nodeName for this analysis.\n",
    "NODES_SCHEMA = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"nodeName\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Ensure the required DataFrames exist and are not empty before proceeding.\n",
    "    if 'sql_df' in locals() and sql_df is not None and sql_df.count() > 0 and 'applications_df' in locals() and applications_df is not None:\n",
    "        logger.info(\"üöÄ Starting Photon compatibility analysis...\")\n",
    "        print(\"üöÄ Starting Photon compatibility analysis...\")\n",
    "\n",
    "        # Step 1: Get the application names from the applications_df DataFrame.\n",
    "        # We select distinct rows to avoid any potential duplicates.\n",
    "        app_names_df = applications_df.select(\n",
    "            \"cluster_id\", \"application_id\", \"application_name\"\n",
    "        ).distinct()\n",
    "\n",
    "        # Step 2: Parse the raw JSON, join with app names, and explode the query plan nodes.\n",
    "        # A left join ensures all SQL queries are kept, even if an app name is missing.\n",
    "        exploded_nodes_df = sql_df.join(\n",
    "            app_names_df,\n",
    "            on=[\"cluster_id\", \"application_id\"],\n",
    "            how=\"left\"\n",
    "        ).withColumn(\n",
    "            \"nodes\",\n",
    "            from_json(col(\"sql_raw_json\"), NODES_SCHEMA)\n",
    "        ).select(\n",
    "            \"cluster_id\",\n",
    "            \"cluster_name\",\n",
    "            \"application_id\",\n",
    "            \"application_name\",\n",
    "            \"sql_id\",\n",
    "            explode(col(\"nodes\")).alias(\"node\")\n",
    "        )\n",
    "\n",
    "        # Step 3: Flag operations that are compatible with Photon.\n",
    "        # A new column 'is_photon_op' is added, with 1 for compatible ops.\n",
    "        photon_check_df = exploded_nodes_df.withColumn(\n",
    "            \"is_photon_op\",\n",
    "            when(col(\"node.nodeName\").isin(NON_PHOTON_OPERATIONS), 0).otherwise(1)\n",
    "        )\n",
    "\n",
    "        # Step 4: Calculate the percentage of compatible operations for each application.\n",
    "        # Group by all identifying fields, including the names, to calculate the score.\n",
    "        photon_analysis_df = photon_check_df.groupBy(\n",
    "            \"cluster_id\", \"cluster_name\", \"application_id\", \"application_name\"\n",
    "        ).agg(\n",
    "            (round(avg(col(\"is_photon_op\")) * 100, 2)).alias(\"photon_compatibility_pct\")\n",
    "        ).orderBy(col(\"photon_compatibility_pct\").desc())\n",
    "\n",
    "        logger.info(\"‚úÖ Photon compatibility analysis complete.\")\n",
    "        print(\"‚úÖ Photon compatibility analysis complete.\")\n",
    "        print(\"üìä Displaying applications ranked by their Photon Compatibility Score:\")\n",
    "\n",
    "        # Step 5: Display the final results, now including the names.\n",
    "        display(photon_analysis_df)\n",
    "\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è 'sql_df' or 'applications_df' is not available or is empty. Skipping Photon analysis.\")\n",
    "        print(\"‚ö†Ô∏è 'sql_df' or 'applications_df' is not available or is empty. Skipping Photon analysis.\")\n",
    "\n",
    "except NameError as ne:\n",
    "    logger.error(\"‚ùå A required DataFrame ('sql_df' or 'applications_df') was not found. Please ensure the main analysis has been run successfully.\", exc_info=True)\n",
    "    print(\"‚ùå A required DataFrame ('sql_df' or 'applications_df') was not found. Please ensure the main analysis has been run successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(\"‚ùå An unexpected error occurred during Photon analysis: %s\", str(e), exc_info=True)\n",
    "    print(f\"‚ùå An unexpected error occurred during Photon analysis: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7368684a-c4a5-4770-9a72-0768502b19f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with raw as (select * from IDENTIFIER(:catalog_name || '.' || :schema_name || '.' || 'sql')\n",
    "lateral view explode(try_variant_get(parse_json(sql_raw_json), '$.nodes', 'array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>>')) as nodemetrics),\n",
    "\n",
    "photoncheck as (select *, case when nodemetrics.nodeName = 'MapElements' then 0 \n",
    "when nodemetrics.nodeName = 'MapPartitions' then 0 \n",
    "when nodemetrics.nodeName = 'Scan csv' then 0\n",
    "when nodemetrics.nodeName = 'Scan json' then 0 \n",
    "when nodemetrics.nodeName = 'PythonUDF' then 0 \n",
    "when nodemetrics.nodeName = 'ScalaUDF' then 0 \n",
    "when nodemetrics.nodeName = 'FlatMapGroupsInPandas' then 0  \n",
    "when nodemetrics.nodeName = 'DeserializeToObject' then 0\n",
    "when nodemetrics.nodeName = 'SerializeFromObject' then 0  \n",
    "else 1 end as photonbinary\n",
    "from raw),\n",
    "\n",
    "jobcheck as (select cluster_name, application_id, try_divide(sum(photonbinary), count(*)) as jobphotonperc \n",
    "from photoncheck \n",
    "group by all)\n",
    "\n",
    "select cluster_name, application_id, any_value(jobphotonperc)\n",
    "from jobcheck\n",
    "group by all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a333b5-0e32-49f1-982b-a6a9d0f4a8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with raw as (select * from IDENTIFIER(:catalog_name || '.' || :schema_name || '.' || 'sql')\n",
    "lateral view explode(try_variant_get(parse_json(sql_raw_json), '$.nodes', 'array<struct<nodeId: INT, nodeName: STRING, metrics: array<struct<name:STRING, value:STRING>>>>')) as nodemetrics),\n",
    "\n",
    "photoncheck as (select *, nodemetrics.nodeName as nodename, case when nodemetrics.nodeName = 'MapElements' then 0 \n",
    "when nodemetrics.nodeName = 'MapPartitions' then 0 \n",
    "when nodemetrics.nodeName = 'Scan csv' then 0\n",
    "when nodemetrics.nodeName = 'Scan json' then 0 \n",
    "when nodemetrics.nodeName = 'PythonUDF' then 0 \n",
    "when nodemetrics.nodeName = 'ScalaUDF' then 0 \n",
    "when nodemetrics.nodeName = 'FlatMapGroupsInPandas' then 0\n",
    "when nodemetrics.nodeName = 'DeserializeToObject' then 0\n",
    "when nodemetrics.nodeName = 'SerializeFromObject' then 0  \n",
    "else 1 end as photonbinary\n",
    "from raw),\n",
    "\n",
    "pu as (select nodename, count(*) as cuenta \n",
    "from photoncheck\n",
    "group by all)\n",
    "\n",
    "select * from pu"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "emr_photon_analysis",
   "widgets": {
    "catalog_name": {
     "currentValue": "prodrs",
     "nuid": "f19f3d85-c0fd-4065-9baf-94cbc4b9466c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "spark_observability",
     "nuid": "28d0b96d-15dd-45ff-a4bd-4b2573a934b6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

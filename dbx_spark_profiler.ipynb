{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303627a0-05ab-446f-8459-3d794dfd27ee",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Boto3 (EMR ONLY)"
    }
   },
   "outputs": [],
   "source": [
    "# #Run cell on cluster restart or if receiving error:\n",
    "# #AttributeError: 'EMR' object has no attribute 'create_persistent_app_ui'\n",
    "# %pip install --upgrade boto3 botocore\n",
    "# %restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fac41074-9105-45bd-87ec-6608ad21de20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Databricks Spark Event Log Analyzer\n",
    "\n",
    "This script analyzes Databricks clusters to extract performance metrics from Spark History Servers. It discovers Databricks clusters, connects to managed Spark History Server UIs, fetches application, job, stage, and SQL query data, and then processes this information into Spark DataFrames for performance analysis and optimization insights.\n",
    "\n",
    "## Required Setup\n",
    "\n",
    "i) Create a secret for token and then fetch token in code (in this example we leverage dbutils with secret scope shscreds but you can name these whatever)\n",
    "\n",
    "ii) Create a secret for data plane URL and then fetch token in code (further instructions in readme)\n",
    "\n",
    "iii) Create a secret for DATAPLANE_DOMAIN_DBAUTH and then fetch token in code (further instructions in readme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb8f1a7-4420-4c73-87bc-f8c0456c5c8a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta, date\n",
    "import logging, os, time\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Configuration Parameters\n",
    "# ----------------------------------------------------------------------\n",
    "# Parse and validate configuration\n",
    "dbutils.widgets.text(\"timeout_seconds\", \"300\", \"Request Timeout (seconds)\")\n",
    "dbutils.widgets.text(\"max_applications\", \"10\", \"Max Applications to Analyze per Cluster\")\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"prod\"], \"Environment (dev/prod)\")\n",
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "dbutils.widgets.text(\"volume_name\", \"profiler_logs_volume\", \"Volume for logs\")\n",
    "dbutils.widgets.text(\"max_clusters\", \"200\", \"Max Clusters to Analyze\")\n",
    "dbutils.widgets.text(\"batch_size\", \"10\", \"Batch Size (clusters to process concurrently)\")\n",
    "dbutils.widgets.text(\"batch_delay_seconds\", \"2\", \"Delay Between Batches (seconds)\")\n",
    "dbutils.widgets.text(\"max_endpoint_failures\", \"3\", \"Max Endpoint Failures per Endpoint Type\")\n",
    "dbutils.widgets.text(\"include_tasks\", \"false\", \"Set to true if you want to include task level metrics\")\n",
    "\n",
    "TIMEOUT_SECONDS = int(dbutils.widgets.get(\"timeout_seconds\") or \"300\")\n",
    "MAX_APPLICATIONS = int(dbutils.widgets.get(\"max_applications\") or \"50\")\n",
    "MAX_CLUSTERS_RAW = int(dbutils.widgets.get(\"max_clusters\") or \"200\")\n",
    "MAX_CLUSTERS = MAX_CLUSTERS_RAW / 20\n",
    "ENVIRONMENT = dbutils.widgets.get(\"environment\").strip()\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip() or DEFAULT_SCHEMA_NAME\n",
    "VOLUME_NAME = dbutils.widgets.get(\"volume_name\").strip() or \"profiler_logs_volume\"\n",
    "BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\") or \"10\")\n",
    "BATCH_DELAY_SECONDS = int(dbutils.widgets.get(\"batch_delay_seconds\") or \"2\")\n",
    "MAX_ENDPOINT_FAILURES = int(dbutils.widgets.get(\"max_endpoint_failures\") or \"3\")\n",
    "TASK_BINARY = dbutils.widgets.get(\"include_tasks\").strip().lower()\n",
    "\n",
    "# UC Validation\n",
    "if not CATALOG_NAME:\n",
    "    raise ValueError(\"catalog widget must point to an existing catalog\")\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\")\n",
    "VOLUME_BASE_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}\"\n",
    "LOG_DIR = f\"{VOLUME_BASE_PATH}\"\n",
    "\n",
    "# Create logs directory\n",
    "try:\n",
    "    os.makedirs(LOG_DIR, exist_ok=True)\n",
    "except Exception:\n",
    "    dbutils.fs.mkdirs(LOG_DIR)\n",
    "\n",
    "# Configure logging\n",
    "run_id = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_FILE = f\"{LOG_DIR}/run-{run_id}.txt\"\n",
    "\n",
    "# Reset handlers to avoid duplicates on re-run\n",
    "for h in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(h)\n",
    "    h.close()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # notebook output\n",
    "        RotatingFileHandler(LOG_FILE, mode='a', encoding=\"utf-8\"),  # single full file\n",
    "    ],\n",
    ")\n",
    "\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"databricks\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# SHS Retry Configuration\n",
    "SHS_RETRY_CONFIG = {\n",
    "    \"retry_settings\": {\n",
    "        \"max_retries\": 3,\n",
    "        \"initial_delay_seconds\": 5.0,\n",
    "        \"max_delay_seconds\": 120.0,\n",
    "        \"backoff_factor\": 2.0,\n",
    "        \"enable_jitter\": True,\n",
    "        \"jitter_max\": 1.0\n",
    "    },\n",
    "    \"timeout_settings\": {\n",
    "        \"connection_timeout\": 30,\n",
    "        \"read_timeout\": 180,\n",
    "        \"total_timeout\": 300\n",
    "    },\n",
    "    \"throttling_settings\": {\n",
    "        \"request_delay_seconds\": 1.0,\n",
    "        \"enable_adaptive_throttling\": True,\n",
    "        \"consecutive_error_threshold\": 5,\n",
    "        \"adaptive_delay_multiplier\": 1.5\n",
    "    },\n",
    "    \"spark_history_server_errors\": {\n",
    "        \"retryable_http_codes\": [429, 500, 502, 503, 504],\n",
    "        \"non_retryable_codes\": [400, 401, 403, 404],\n",
    "        \"connection_errors\": [\"ConnectionError\", \"Timeout\", \"HTTPError\"]\n",
    "    },\n",
    "    \"endpoint_specific_settings\": {\n",
    "        \"stages\": {\n",
    "            \"max_retries\": 2,\n",
    "            \"read_timeout\": 600\n",
    "        },\n",
    "        \"jobs\": {\n",
    "            \"max_retries\": 2,\n",
    "            \"read_timeout\": 600\n",
    "        },\n",
    "        \"sql\": {\n",
    "            \"max_retries\": 2,\n",
    "            \"read_timeout\": 600\n",
    "        },\n",
    "        \"executors\": {\n",
    "            \"max_retries\": 2,\n",
    "            \"read_timeout\": 300\n",
    "        },\n",
    "        \"applications\": {\n",
    "            \"max_retries\": 2,\n",
    "            \"read_timeout\": 300\n",
    "        }\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"enable_retry_logging\": True,\n",
    "        \"enable_success_logging\": True,\n",
    "        \"enable_throttle_logging\": True,\n",
    "        \"log_shs_url\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Log final configuration\n",
    "print(\"Configuration:\")\n",
    "print(f\" Environment: {ENVIRONMENT}\")\n",
    "print(f\" Timeout: {TIMEOUT_SECONDS} seconds\")\n",
    "print(f\" Max Applications per Cluster: {MAX_APPLICATIONS}\")\n",
    "print(f\" Max Clusters to Analyze: {MAX_CLUSTERS}\")\n",
    "print(f\" Catalog: {CATALOG_NAME}\")\n",
    "print(f\" Schema: {SCHEMA_NAME}\")\n",
    "print(f\" Log file: {LOG_FILE}\")\n",
    "print(f\" Batch Size: {BATCH_SIZE} clusters\")\n",
    "print(f\" Batch Delay: {BATCH_DELAY_SECONDS} seconds\")\n",
    "print(f\" Max Endpoint Failures: {MAX_ENDPOINT_FAILURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03466bf-eeac-4097-969a-74984cda3a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DBX Cluster Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3ef9d92-72d9-483a-b9d8-2aeca212afc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "DBX Cluster Discovery"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "class DBXClusterDiscovery:\n",
    "    \"\"\"Discovery and management of EMR clusters.\"\"\"\n",
    "\n",
    "    def __init__(self, region: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the EMR cluster discovery client.\n",
    "\n",
    "        :param region: AWS region.\n",
    "        :raises TypeError: If region is not a non-empty string.\n",
    "        \"\"\"\n",
    "        service = 'Databricks'\n",
    "\n",
    "    def discover_clusters(\n",
    "        self,\n",
    "        states: Optional[List[str]] = None,\n",
    "        name_filter: Optional[str] = None,\n",
    "        max_clusters: int = 100,\n",
    "        created_after: Optional[datetime] = None,\n",
    "        created_before: Optional[datetime] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Discover EMR clusters based on criteria.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "        \n",
    "            token = dbutils.secrets.get(scope=\"shscreds\", key=\"token\")\n",
    "            databricks_workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)\n",
    "            token_header = {'Authorization': 'Bearer {0}'.format(token)}\n",
    "            initurl = f'{databricks_workspace_url}/api/2.1/clusters/list?filter_by.cluster_sources=JOB&filter_by.cluster_states=TERMINATED'\n",
    "            allclusters = []\n",
    "            nextpage = 'init'\n",
    "            i = 0\n",
    "\n",
    "\n",
    "            while nextpage != \"\" and i < MAX_CLUSTERS:\n",
    "                if nextpage == 'init':\n",
    "                    resp = requests.get(initurl, headers=token_header)\n",
    "                    respjson = resp.json()\n",
    "                else:\n",
    "                    url = f\"{databricks_workspace_url}/api/2.1/clusters/list?filter_by.cluster_sources=JOB&filter_by.cluster_states=TERMINATED&page_token={nextpage}\"\n",
    "                    resp = requests.get(url, headers=token_header)\n",
    "                    respjson = resp.json()\n",
    "                pageofclusters = respjson.get('clusters')\n",
    "                for cluster in pageofclusters:\n",
    "                    clusterid = cluster.get('cluster_id')\n",
    "                    clustername = cluster.get('cluster_name')\n",
    "                    clustersource = cluster.get('cluster_source')\n",
    "                    state = cluster.get('state')\n",
    "                    sparkcontextid = cluster.get('spark_context_id')\n",
    "                    tmpdict = {'cluster_id': clusterid, 'cluster_name': clustername, 'cluster_source': clustersource, 'state': state, 'spark_context_id': sparkcontextid}\n",
    "                    allclusters.append(tmpdict)\n",
    "                nextpage = respjson.get('next_page_token')\n",
    "                i = i + 1\n",
    "\n",
    "            logger.info(\"✅ Discovered %s clusters\", len(allclusters))\n",
    "            return allclusters\n",
    "        except ClientError as e:\n",
    "            logger.error(\"❌ Failed to discover clusters: %s\", e.response[\"Error\"][\"Message\"], exc_info=True)\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0de5f7f0-aa9d-4e7f-9b8e-7ba31d015f02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark History Server REST Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f742cba-d8de-46b6-8a03-6838e6616971",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SHS REST Interactions"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import time\n",
    "from typing import List, Any, Dict, Optional\n",
    "import requests\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SparkHistoryServerClient:\n",
    "    \"\"\"Client for interacting with Spark History Server REST API.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, session: requests.Session, cookies: str):\n",
    "        \"\"\"\n",
    "        Initialize the Spark History Server client.\n",
    "\n",
    "        :param base_url: Base URL for the Spark History Server.\n",
    "        :param session: Configured HTTP session with authentication.\n",
    "        \"\"\"\n",
    "        if not base_url or not isinstance(base_url, str):\n",
    "            raise ValueError(\"base_url must be a non-empty string.\")\n",
    "        if not isinstance(session, requests.Session):\n",
    "            raise TypeError(\"session must be a requests.Session object.\")\n",
    "            \n",
    "        self.base_url = base_url\n",
    "        self.session = session\n",
    "        self.cookies = cookies\n",
    "        self.api_base = f\"{base_url}api/v1\"\n",
    "        self.retry_config = SHS_RETRY_CONFIG\n",
    "        self.consecutive_errors = {}  # Track consecutive errors per endpoint\n",
    "        self.last_request_time = 0  # For throttling\n",
    "\n",
    "    def _calculate_jitter(self, delay: float) -> float:\n",
    "        \"\"\"Calculate jitter for backoff delay.\"\"\"\n",
    "        if not self.retry_config[\"retry_settings\"][\"enable_jitter\"]:\n",
    "            return delay\n",
    "        \n",
    "        jitter_max = self.retry_config[\"retry_settings\"][\"jitter_max\"]\n",
    "        jitter = random.uniform(0, jitter_max)\n",
    "        return delay + jitter\n",
    "\n",
    "    def _get_endpoint_type(self, endpoint: str) -> str:\n",
    "        \"\"\"Extract endpoint type from endpoint path for configuration lookup.\"\"\"\n",
    "        if '/sql' in endpoint:\n",
    "            return 'sql'\n",
    "        elif '/stages' in endpoint:\n",
    "            return 'stages'\n",
    "        elif '/jobs' in endpoint:\n",
    "            return 'jobs'\n",
    "        elif '/allexecutors' in endpoint:\n",
    "            return 'executors'\n",
    "        elif 'applications' in endpoint and not any(x in endpoint for x in ['/jobs', '/stages', '/sql', '/allexecutors']):\n",
    "            return 'applications'\n",
    "        else:\n",
    "            return 'default'\n",
    "\n",
    "    def _apply_throttling(self, endpoint_type: str):\n",
    "        \"\"\"Apply throttling based on configuration and consecutive errors.\"\"\"\n",
    "        config = self.retry_config\n",
    "        throttling = config[\"throttling_settings\"]\n",
    "        \n",
    "        # Base request delay\n",
    "        base_delay = throttling[\"request_delay_seconds\"]\n",
    "        \n",
    "        # Adaptive throttling based on consecutive errors\n",
    "        if throttling[\"enable_adaptive_throttling\"]:\n",
    "            consecutive_errors = self.consecutive_errors.get(endpoint_type, 0)\n",
    "            if consecutive_errors >= throttling[\"consecutive_error_threshold\"]:\n",
    "                adaptive_delay = base_delay * (throttling[\"adaptive_delay_multiplier\"] ** (consecutive_errors - throttling[\"consecutive_error_threshold\"] + 1))\n",
    "                base_delay = min(adaptive_delay, config[\"retry_settings\"][\"max_delay_seconds\"])\n",
    "                \n",
    "                if config[\"logging\"][\"enable_throttle_logging\"]:\n",
    "                    logger.info(\"Applying adaptive throttling for %s: %d consecutive errors, delay: %.2fs\", \n",
    "                              endpoint_type, consecutive_errors, base_delay)\n",
    "        \n",
    "        # Ensure minimum time between requests\n",
    "        time_since_last = time.time() - self.last_request_time\n",
    "        if time_since_last < base_delay:\n",
    "            sleep_time = base_delay - time_since_last\n",
    "            if config[\"logging\"][\"enable_throttle_logging\"]:\n",
    "                logger.info(\"Throttling request for %s: sleeping %.2fs\", endpoint_type, sleep_time)\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "\n",
    "    def _is_retryable_error(self, exception: Exception) -> bool:\n",
    "        \"\"\"Determine if an error is retryable based on configuration.\"\"\"\n",
    "        error_config = self.retry_config[\"spark_history_server_errors\"]\n",
    "        \n",
    "        if isinstance(exception, requests.exceptions.HTTPError):\n",
    "            status_code = exception.response.status_code\n",
    "            if status_code in error_config[\"non_retryable_codes\"]:\n",
    "                return False\n",
    "            return status_code in error_config[\"retryable_http_codes\"]\n",
    "        \n",
    "        # Check connection errors\n",
    "        exception_name = type(exception).__name__\n",
    "        return exception_name in error_config[\"connection_errors\"]\n",
    "\n",
    "    def _make_request(self, endpoint: str, params: Optional[Dict] = None, max_retries: int = None) -> Any:\n",
    "        \"\"\"\n",
    "        Make a REST API request with enhanced retry logic based on SHS_RETRY_CONFIG.\n",
    "\n",
    "        :param endpoint: The API endpoint to call (e.g., 'applications/app-123/jobs').\n",
    "        :param params: A dictionary of query parameters for the request.\n",
    "        :param max_retries: Optional override for max retries (uses config if None).\n",
    "        :returns: The JSON response from the API.\n",
    "        :raises requests.exceptions.RequestException: If the request fails after all retries.\n",
    "        \"\"\"\n",
    "        url = f\"{self.api_base}/{endpoint}\"\n",
    "        #print(f\"!!!!! URL {url} HERE\")\n",
    "        endpoint_type = self._get_endpoint_type(endpoint)\n",
    "        config = self.retry_config\n",
    "        cookies = self.cookies\n",
    "        \n",
    "        # Get endpoint-specific settings\n",
    "        endpoint_settings = config[\"endpoint_specific_settings\"].get(endpoint_type, {})\n",
    "        \n",
    "        # Determine max retries (endpoint-specific > parameter > global config)\n",
    "        if max_retries is None:\n",
    "            max_retries = endpoint_settings.get(\"max_retries\", config[\"retry_settings\"][\"max_retries\"])\n",
    "        \n",
    "        # Determine timeouts\n",
    "        connection_timeout = config[\"timeout_settings\"][\"connection_timeout\"]\n",
    "        read_timeout = endpoint_settings.get(\"read_timeout\", config[\"timeout_settings\"][\"read_timeout\"])\n",
    "        timeout = (connection_timeout, read_timeout)\n",
    "        \n",
    "        if config[\"logging\"][\"log_shs_url\"] and config[\"logging\"][\"enable_retry_logging\"]:\n",
    "            logger.info(\"Making request to: %s (endpoint_type: %s, max_retries: %d)\", url, endpoint_type, max_retries)\n",
    "        \n",
    "        # Apply throttling before making request\n",
    "        self._apply_throttling(endpoint_type)\n",
    "        \n",
    "        last_exception = None\n",
    "        \n",
    "        # The retry loop starts with attempt 0 (the first try)\n",
    "        for attempt in range(max_retries + 1):\n",
    "            if attempt > 0:\n",
    "                # Calculate backoff delay with jitter\n",
    "                base_delay = config[\"retry_settings\"][\"initial_delay_seconds\"] * (\n",
    "                    config[\"retry_settings\"][\"backoff_factor\"] ** (attempt - 1)\n",
    "                )\n",
    "                delay = min(base_delay, config[\"retry_settings\"][\"max_delay_seconds\"])\n",
    "                delay_with_jitter = self._calculate_jitter(delay)\n",
    "                \n",
    "                if config[\"logging\"][\"enable_retry_logging\"]:\n",
    "                    logger.info(\"Retrying %s in %.2f seconds (attempt %d/%d)...\", \n",
    "                              endpoint, delay_with_jitter, attempt, max_retries)\n",
    "                \n",
    "                time.sleep(delay_with_jitter)\n",
    "\n",
    "            try:\n",
    "                response = self.session.get(url, cookies=cookies, params=params, timeout=timeout, allow_redirects=True)\n",
    "                #response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "                \n",
    "                # Reset consecutive errors on success\n",
    "                self.consecutive_errors[endpoint_type] = 0\n",
    "                \n",
    "                if config[\"logging\"][\"enable_success_logging\"]:\n",
    "                    logger.debug(\"✅ Successfully retrieved data from %s (attempt %d)\", endpoint, attempt + 1)\n",
    "                \n",
    "                return response.json()\n",
    "                \n",
    "            except Exception as e:\n",
    "                last_exception = e\n",
    "                \n",
    "                # Track consecutive errors for adaptive throttling\n",
    "                self.consecutive_errors[endpoint_type] = self.consecutive_errors.get(endpoint_type, 0) + 1\n",
    "                \n",
    "                # Check if error is retryable\n",
    "                #if not self._is_retryable_error(e):\n",
    "                #    if config[\"logging\"][\"enable_retry_logging\"]:\n",
    "                #        logger.error(\"❌ Non-retryable error for %s: %s\", url, str(e))\n",
    "                #    raise e\n",
    "                \n",
    "                #if config[\"logging\"][\"enable_retry_logging\"]:\n",
    "                #    logger.warning(\"Retryable error on attempt %d for %s: %s\", attempt + 1, url, str(e))\n",
    "                \n",
    "                # If this was the last attempt, don't continue\n",
    "                if attempt == max_retries:\n",
    "                    break\n",
    "\n",
    "        # All retries exhausted\n",
    "        if config[\"logging\"][\"enable_retry_logging\"]:\n",
    "            logger.error(\"All %d retry attempts failed for %s.\", max_retries + 1, url)\n",
    "        \n",
    "        raise last_exception\n",
    "\n",
    "    def get_applications(self, status: Optional[str] = None, limit: int = 100) -> List[Dict]:\n",
    "        logger.info(\"Fetching applications (status: %s, limit: %s)\", status, limit)\n",
    "        params = {'limit': limit}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "        return self._make_request(\"applications\", params)\n",
    "\n",
    "    def get_application_details(self, app_id: str) -> Dict:\n",
    "        logger.info(\"Fetching application details for: %s\", app_id)\n",
    "        return self._make_request(f\"applications/{app_id}\")\n",
    "\n",
    "    def get_application_jobs(self, app_id: str, attempt_id: Optional[str] = None, status: Optional[str] = None) -> List[Dict]:\n",
    "        endpointog = f\"applications/{app_id}/{attempt_id}/jobs\" if attempt_id else f\"applications/{app_id}/jobs\"\n",
    "        endpoint = f\"applications/{app_id}/jobs\"\n",
    "        params = {'status': status} if status else {}\n",
    "        return self._make_request(endpoint, params)\n",
    "\n",
    "    def get_application_stages(self, app_id: str, attempt_id: Optional[str] = None, status: Optional[str] = None) -> List[Dict]:\n",
    "        endpointog = f\"applications/{app_id}/{attempt_id}/stages\" if attempt_id else f\"applications/{app_id}/stages\"\n",
    "        endpoint = f\"applications/{app_id}/stages\"\n",
    "        params = {'details': 'true', 'withSummaries': 'true'}\n",
    "        if status:\n",
    "            params['status'] = status\n",
    "        return self._make_request(endpoint, params)\n",
    "\n",
    "    def get_stage_tasks(self, app_id: str, attempt_id: str, stage_id: int, stage_attempt: int = 0) -> List[Dict]:\n",
    "        endpoint = f\"applications/{app_id}/{attempt_id}/stages/{stage_id}/{stage_attempt}/taskList\"\n",
    "        return self._make_request(endpoint)\n",
    "\n",
    "    def get_stage_task_summary(self, app_id: str, attempt_id: str, stage_id: int, stage_attempt: int = 0) -> Dict:\n",
    "        endpoint = f\"applications/{app_id}/{attempt_id}/stages/{stage_id}/{stage_attempt}/taskSummary\"\n",
    "        params = {'quantiles': \"0.0,0.25,0.5,0.75,1.0\"}\n",
    "        return self._make_request(endpoint, params)\n",
    "\n",
    "    def get_application_executors(self, app_id: str, attempt_id: Optional[str] = None) -> List[Dict]:\n",
    "        endpoint = f\"applications/{app_id}/{attempt_id}/allexecutors\" if attempt_id else f\"applications/{app_id}/allexecutors\"\n",
    "        return self._make_request(endpoint)\n",
    "\n",
    "    def get_application_sql_queries(self, app_id: str, attempt_id: Optional[str] = None) -> List[Dict]:\n",
    "        endpointog = f\"applications/{app_id}/{attempt_id}/sql\" if attempt_id else f\"applications/{app_id}/sql\"\n",
    "        endpoint = f\"applications/{app_id}/sql\"\n",
    "        params = {'details': 'true', 'planDescription': 'true'}\n",
    "        return self._make_request(endpoint, params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868055bc-d1c1-475d-a8dc-10df3fbe6d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Metrics Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f700d0f4-c4c8-4142-a2c9-3247b95de9e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SHS Metric Analysis"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, LongType, DoubleType, BooleanType\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SparkMetricsAnalyzer:\n",
    "    \"\"\"Analyzer for Spark application metrics and performance data.\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Initialize the metrics analyzer.\n",
    "\n",
    "        :param spark: Spark session instance for DataFrame creation.\n",
    "        :raises TypeError: If spark is not a SparkSession instance.\n",
    "        \"\"\"\n",
    "        #if not isinstance(spark, SparkSession):\n",
    "        #    raise TypeError(\"Parameter 'spark' must be a SparkSession instance\")\n",
    "        self.spark = spark\n",
    "\n",
    "    def analyze_application_performance(self, app_data: Dict, latest_attempt_id: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for a single application.\n",
    "\n",
    "        :param app_data: Application data from Spark History Server.\n",
    "        :param latest_attempt_id: The latest attempt ID to extract data from.\n",
    "        :returns: Performance analysis results dictionary.\n",
    "        \"\"\"\n",
    "        latest_attempt_info = next((a for a in app_data.get('attempts', []) if a.get('attemptId') == latest_attempt_id), {})\n",
    "        source_data = latest_attempt_info or app_data\n",
    "        duration_ms = source_data.get('duration', 0)\n",
    "\n",
    "        return {\n",
    "            'application_id': app_data.get('id'),\n",
    "            'application_name': app_data.get('name', 'Unknown'),\n",
    "            'duration_ms': duration_ms,\n",
    "            'duration_minutes': round(duration_ms / 60000, 2),\n",
    "            'start_time': source_data.get('startTime'),\n",
    "            'end_time': source_data.get('endTime'),\n",
    "            'spark_version': latest_attempt_info.get('appSparkVersion') or app_data.get('sparkVersion', 'Unknown'),\n",
    "        }\n",
    "\n",
    "    def analyze_job_performance(self, jobs: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for jobs.\n",
    "\n",
    "        :param jobs: List of job data dictionaries.\n",
    "        :returns: List of job performance analysis dictionaries.\n",
    "        \"\"\"\n",
    "        job_analysis = []\n",
    "        for job in jobs:\n",
    "            num_tasks = job.get('numTasks', 0)\n",
    "            num_completed = job.get('numCompletedTasks', 0)\n",
    "            analysis = {\n",
    "                'job_id': job.get('jobId'),\n",
    "                'job_name': job.get('name', 'Unknown'),\n",
    "                'status': job.get('status', 'UNKNOWN'),\n",
    "                'submission_time': job.get('submissionTime'),\n",
    "                'completion_time': job.get('completionTime'),\n",
    "                'num_tasks': num_tasks,\n",
    "                'num_completed_tasks': num_completed,\n",
    "                'num_failed_tasks': job.get('numFailedTasks', 0),\n",
    "                'stage_ids': str(job.get('stageIds', [])),\n",
    "                'task_success_rate': round((num_completed / num_tasks) * 100.0, 2) if num_tasks > 0 else 0.0,\n",
    "            }\n",
    "            job_analysis.append(analysis)\n",
    "        return job_analysis\n",
    "\n",
    "    def analyze_stage_performance(self, stages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for stages.\n",
    "\n",
    "        :param stages: List of stage data dictionaries.\n",
    "        :returns: List of stage performance analysis dictionaries.\n",
    "        \"\"\"\n",
    "        stage_analysis = []\n",
    "        for stage in stages:\n",
    "            analysis = {\n",
    "                'stage_id': stage.get('stageId'),\n",
    "                'stage_name': stage.get('name') or 'Unknown',\n",
    "                'status': stage.get('status') or 'UNKNOWN',\n",
    "                'num_tasks': stage.get('numTasks') or 0,\n",
    "                'num_active_tasks': stage.get('numActiveTasks') or 0,\n",
    "                'num_complete_tasks': stage.get('numCompleteTasks') or 0,\n",
    "                'num_failed_tasks': stage.get('numFailedTasks') or 0,\n",
    "                'executor_run_time': stage.get('executorRunTime') or 0,\n",
    "                'executor_cpu_time': stage.get('executorCpuTime') or 0,\n",
    "                'submission_time': stage.get('submissionTime'),\n",
    "                'first_task_launched_time': stage.get('firstTaskLaunchedTime'),\n",
    "                'completion_time': stage.get('completionTime'),\n",
    "                'input_bytes': stage.get('inputBytes') or 0,\n",
    "                'output_bytes': stage.get('outputBytes') or 0,\n",
    "                'shuffle_read_bytes': stage.get('shuffleReadBytes') or 0,\n",
    "                'shuffle_write_bytes': stage.get('shuffleWriteBytes') or 0,\n",
    "                'memory_bytes_spilled': stage.get('memoryBytesSpilled') or 0,\n",
    "                'disk_bytes_spilled': stage.get('diskBytesSpilled') or 0,\n",
    "                'task_completion_rate': 0.0,\n",
    "                'avg_executor_run_time_per_task': 0.0,\n",
    "                'total_data_processed_mb': 0.0,\n",
    "                'shuffle_data_mb': 0.0,\n",
    "                'cluster_id': '',  # Will be populated by caller\n",
    "                'cluster_name': '',  # Will be populated by caller\n",
    "                'application_id': ''  # Will be populated by caller\n",
    "            }\n",
    "\n",
    "            # Calculate efficiency metrics with safe division\n",
    "            num_tasks = analysis['num_tasks']\n",
    "            num_complete = analysis['num_complete_tasks']\n",
    "            executor_run_time = analysis['executor_run_time']\n",
    "            \n",
    "            if num_tasks > 0:\n",
    "                analysis['task_completion_rate'] = round((num_complete / num_tasks) * 100.0, 2)\n",
    "                if executor_run_time > 0:\n",
    "                    analysis['avg_executor_run_time_per_task'] = round(executor_run_time / num_tasks, 2)\n",
    "\n",
    "            # Calculate data processing metrics (convert bytes to MB)\n",
    "            input_bytes = analysis['input_bytes']\n",
    "            output_bytes = analysis['output_bytes']\n",
    "            shuffle_read_bytes = analysis['shuffle_read_bytes']\n",
    "            shuffle_write_bytes = analysis['shuffle_write_bytes']\n",
    "            \n",
    "            analysis['total_data_processed_mb'] = round((input_bytes + output_bytes) / (1024 * 1024), 2)\n",
    "            analysis['shuffle_data_mb'] = round((shuffle_read_bytes + shuffle_write_bytes) / (1024 * 1024), 2)\n",
    "\n",
    "            stage_analysis.append(analysis)\n",
    "\n",
    "        return stage_analysis\n",
    "\n",
    "    def analyze_task_performance(self, tasks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for tasks.\n",
    "\n",
    "        :param tasks: List of task data dictionaries.\n",
    "        :returns: List of task performance analysis dictionaries.\n",
    "        \"\"\"\n",
    "        task_analysis = []\n",
    "        for task in tasks:\n",
    "            analysis = {\n",
    "                'task_id': task.get('taskId'),\n",
    "                'index': task.get('index'),\n",
    "                'attempt': task.get('attempt'),\n",
    "                'launch_time': task.get('launchTime'),\n",
    "                'duration': task.get('duration'),\n",
    "                'executor_id': task.get('executorId'),\n",
    "                'host': task.get('host'),\n",
    "                'status': task.get('status'),\n",
    "                'task_locality': task.get('taskLocality'),\n",
    "                'speculative': task.get('speculative'),\n",
    "                'stage_id': task.get('stage_id'),\n",
    "                'stage_attempt_id': task.get('stage_attempt_id'),\n",
    "            }\n",
    "            task_analysis.append(analysis)\n",
    "        return task_analysis\n",
    "\n",
    "    def analyze_sql_queries(self, sql_queries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze SQL query metrics.\n",
    "\n",
    "        :param sql_queries: List of SQL query data dictionaries.\n",
    "        :returns: List of processed SQL query analysis dictionaries.\n",
    "        \"\"\"\n",
    "        sql_analysis = []\n",
    "        for query in sql_queries:\n",
    "            analysis = {\n",
    "                'sql_id': query.get(\"id\"),\n",
    "                'description': query.get(\"description\", \"N/A\"),\n",
    "                'status': query.get(\"status\"),\n",
    "                'duration_ms': query.get(\"duration\", 0),\n",
    "                'submission_time': query.get(\"submissionTime\"),\n",
    "                'sql_raw_json': json.dumps(query),\n",
    "            }\n",
    "            sql_analysis.append(analysis)\n",
    "        return sql_analysis\n",
    "\n",
    "    def analyze_executor_performance(self, executors: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze performance metrics for executors.\n",
    "\n",
    "        :param executors: List of executor data dictionaries from Spark History Server.\n",
    "        :returns: List of executor performance analysis dictionaries.\n",
    "        \"\"\"\n",
    "        executor_analysis = []\n",
    "        for executor in executors:\n",
    "            analysis = {\n",
    "                'executor_id': executor.get('id'),\n",
    "                'host_port': executor.get('hostPort'),\n",
    "                'is_active': executor.get('isActive'),\n",
    "                'rdd_blocks': executor.get('rddBlocks'),\n",
    "                'memory_used': executor.get('memoryUsed'),\n",
    "                'disk_used': executor.get('diskUsed'),\n",
    "                'total_cores': executor.get('totalCores'),\n",
    "                'max_tasks': executor.get('maxTasks'),\n",
    "                'active_tasks': executor.get('activeTasks'),\n",
    "                'failed_tasks': executor.get('failedTasks'),\n",
    "                'completed_tasks': executor.get('completedTasks'),\n",
    "                'total_tasks': executor.get('totalTasks'),\n",
    "                'total_duration': executor.get('totalDuration'),\n",
    "                'total_gc_time': executor.get('totalGCTime'),\n",
    "                'total_input_bytes': executor.get('totalInputBytes'),\n",
    "                'total_shuffle_read': executor.get('totalShuffleRead'),\n",
    "                'total_shuffle_write': executor.get('totalShuffleWrite'),\n",
    "                'is_blacklisted': executor.get('isBlacklisted', False),\n",
    "                'max_memory': executor.get('maxMemory'),\n",
    "                'add_time': executor.get('addTime'),\n",
    "                'executor_logs': json.dumps(executor.get('executorLogs', {})),\n",
    "            }\n",
    "            executor_analysis.append(analysis)\n",
    "        return executor_analysis\n",
    "\n",
    "    def analyze_task_summaries(self, task_summaries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Analyze task summary metrics.\n",
    "\n",
    "        :param task_summaries: List of task summary data dictionaries.\n",
    "        :returns: List of processed task summary analysis dictionaries.\n",
    "        \"\"\"\n",
    "        analysis_list = []\n",
    "        for summary in task_summaries:\n",
    "            analysis = {\n",
    "                'application_id': summary.get('application_id'),\n",
    "                'stage_id': summary.get('stage_id'),\n",
    "                'stage_attempt_id': summary.get('stage_attempt_id'),\n",
    "                'raw_json': json.dumps(summary),\n",
    "            }\n",
    "            analysis_list.append(analysis)\n",
    "        return analysis_list\n",
    "\n",
    "    def create_dynamic_dataframes(\n",
    "        self,\n",
    "        applications_analysis: List[Dict],\n",
    "        jobs_analysis: List[Dict],\n",
    "        stages_analysis: List[Dict],\n",
    "        tasks_analysis: List[Dict],\n",
    "        sql_analysis: List[Dict],\n",
    "        executors_analysis: List[Dict],\n",
    "        task_summaries_analysis: List[Dict]\n",
    "        ) -> Tuple[Optional[DataFrame], Optional[DataFrame], Optional[DataFrame], Optional[DataFrame], Optional[DataFrame], Optional[DataFrame], Optional[DataFrame]]:\n",
    "        \"\"\"\n",
    "        Create Spark DataFrames from analysis results with explicit, well-defined schemas.\n",
    "        This version includes the 'attempt_id' field to uniquely identify data from each application run.\n",
    "        \"\"\"\n",
    "        def create_df(data: List[Dict], schema: StructType, name: str) -> Optional[DataFrame]:\n",
    "            if not data:\n",
    "                logger.info(\"No data provided for %s DataFrame.\", name)\n",
    "                return None\n",
    "            try:\n",
    "                df = self.spark.createDataFrame(data, schema=schema)\n",
    "                logger.info(\"✅ Created %s DataFrame with %d rows.\", name, df.count())\n",
    "                return df\n",
    "            except Exception as e:\n",
    "                logger.error(\"❌ Failed to create %s DataFrame: %s\", name, str(e), exc_info=True)\n",
    "                return None\n",
    "\n",
    "        # --- Updated Schemas with attempt_id ---\n",
    "        applications_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"application_name\", StringType(), True), StructField(\"duration_ms\", LongType(), True),\n",
    "            StructField(\"duration_minutes\", DoubleType(), True), StructField(\"start_time\", StringType(), True),\n",
    "            StructField(\"end_time\", StringType(), True), StructField(\"spark_version\", StringType(), True)\n",
    "        ])\n",
    "        jobs_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"job_id\", LongType(), True), StructField(\"job_name\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True), StructField(\"submission_time\", StringType(), True),\n",
    "            StructField(\"completion_time\", StringType(), True), StructField(\"num_tasks\", LongType(), True),\n",
    "            StructField(\"num_completed_tasks\", LongType(), True), StructField(\"num_failed_tasks\", LongType(), True),\n",
    "            StructField(\"stage_ids\", StringType(), True), StructField(\"task_success_rate\", DoubleType(), True)\n",
    "        ])\n",
    "        stages_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"stage_id\", LongType(), True), StructField(\"stage_name\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True), StructField(\"num_tasks\", LongType(), True),\n",
    "            StructField(\"num_active_tasks\", LongType(), True), StructField(\"num_complete_tasks\", LongType(), True),\n",
    "            StructField(\"num_failed_tasks\", LongType(), True), StructField(\"executor_run_time\", LongType(), True),\n",
    "            StructField(\"executor_cpu_time\", LongType(), True), StructField(\"submission_time\", StringType(), True),\n",
    "            StructField(\"first_task_launched_time\", StringType(), True), StructField(\"completion_time\", StringType(), True),\n",
    "            StructField(\"input_bytes\", LongType(), True), StructField(\"output_bytes\", LongType(), True),\n",
    "            StructField(\"shuffle_read_bytes\", LongType(), True), StructField(\"shuffle_write_bytes\", LongType(), True),\n",
    "            StructField(\"memory_bytes_spilled\", LongType(), True), StructField(\"disk_bytes_spilled\", LongType(), True),\n",
    "            StructField(\"task_completion_rate\", DoubleType(), True), StructField(\"avg_executor_run_time_per_task\", DoubleType(), True),\n",
    "            StructField(\"total_data_processed_mb\", DoubleType(), True), StructField(\"shuffle_data_mb\", DoubleType(), True)\n",
    "        ])\n",
    "        tasks_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"stage_id\", LongType(), True), StructField(\"stage_attempt_id\", LongType(), True),\n",
    "            StructField(\"task_id\", LongType(), True), StructField(\"index\", LongType(), True),\n",
    "            StructField(\"attempt\", LongType(), True), StructField(\"launch_time\", StringType(), True),\n",
    "            StructField(\"duration\", LongType(), True), StructField(\"executor_id\", StringType(), True),\n",
    "            StructField(\"host\", StringType(), True), StructField(\"status\", StringType(), True),\n",
    "            StructField(\"task_locality\", StringType(), True), StructField(\"speculative\", BooleanType(), True)\n",
    "        ])\n",
    "        sql_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"sql_id\", LongType(), True), StructField(\"description\", StringType(), True),\n",
    "            StructField(\"status\", StringType(), True), StructField(\"duration_ms\", LongType(), True),\n",
    "            StructField(\"submission_time\", StringType(), True), StructField(\"sql_raw_json\", StringType(), True)\n",
    "        ])\n",
    "        executors_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"executor_id\", StringType(), True), StructField(\"host_port\", StringType(), True),\n",
    "            StructField(\"is_active\", BooleanType(), True), StructField(\"rdd_blocks\", LongType(), True),\n",
    "            StructField(\"memory_used\", LongType(), True), StructField(\"disk_used\", LongType(), True),\n",
    "            StructField(\"total_cores\", LongType(), True), StructField(\"max_tasks\", LongType(), True),\n",
    "            StructField(\"active_tasks\", LongType(), True), StructField(\"failed_tasks\", LongType(), True),\n",
    "            StructField(\"completed_tasks\", LongType(), True), StructField(\"total_tasks\", LongType(), True),\n",
    "            StructField(\"total_duration\", LongType(), True), StructField(\"total_gc_time\", LongType(), True),\n",
    "            StructField(\"total_input_bytes\", LongType(), True), StructField(\"total_shuffle_read\", LongType(), True),\n",
    "            StructField(\"total_shuffle_write\", LongType(), True), StructField(\"is_blacklisted\", BooleanType(), True),\n",
    "            StructField(\"max_memory\", LongType(), True), StructField(\"add_time\", StringType(), True),\n",
    "            StructField(\"executor_logs\", StringType(), True)\n",
    "        ])\n",
    "        task_summaries_schema = StructType([\n",
    "            StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True),\n",
    "            StructField(\"application_id\", StringType(), True), StructField(\"attempt_id\", StringType(), True),\n",
    "            StructField(\"stage_id\", LongType(), True), StructField(\"stage_attempt_id\", LongType(), True),\n",
    "            StructField(\"raw_json\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Create DataFrames\n",
    "        apps_df = create_df(applications_analysis, applications_schema, \"applications\")\n",
    "        jobs_df = create_df(jobs_analysis, jobs_schema, \"jobs\")\n",
    "        stages_df = create_df(stages_analysis, stages_schema, \"stages\")\n",
    "        tasks_df = create_df(tasks_analysis, tasks_schema, \"tasks\")\n",
    "        sql_df = create_df(sql_analysis, sql_schema, \"sql\")\n",
    "        executors_df = create_df(executors_analysis, executors_schema, \"executors\")\n",
    "        task_summaries_df = create_df(task_summaries_analysis, task_summaries_schema, \"task_summaries\")\n",
    "\n",
    "        return apps_df, jobs_df, stages_df, tasks_df, sql_df, executors_df, task_summaries_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2de8910b-c795-461a-9078-43b914b4f8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Cluster Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b9d400-9d8e-43da-907f-1b0a41a116bd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Orchestration"
    }
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# A private constant to limit the number of errors recorded per endpoint to avoid excessive memory usage.\n",
    "_MAX_ERRORS_PER_ENDPOINT = 3\n",
    "\n",
    "def _truncate_message(message: str, max_len: int = 500) -> str:\n",
    "    \"\"\"Truncates a message to a maximum length.\"\"\"\n",
    "    return message if len(message) <= max_len else message[:max_len-3] + \"...\"\n",
    "\n",
    "def _classify_exception_code(exc: Exception) -> str:\n",
    "    import requests\n",
    "    try:\n",
    "        from botocore.exceptions import ClientError\n",
    "    except Exception:\n",
    "        ClientError = tuple()\n",
    "\n",
    "    msg = str(exc) if exc else \"\"\n",
    "\n",
    "    # Request-level errors\n",
    "    if isinstance(exc, (requests.exceptions.ReadTimeout, requests.exceptions.Timeout)):\n",
    "        return \"TIMEOUT\"\n",
    "    if isinstance(exc, requests.exceptions.ConnectionError):\n",
    "        return \"CONNECTION_ERROR\"\n",
    "    if isinstance(exc, requests.exceptions.HTTPError) and getattr(exc, \"response\", None):\n",
    "        try:\n",
    "            return f\"HTTP_{exc.response.status_code}\"\n",
    "        except Exception:\n",
    "            return \"HTTP_ERROR\"\n",
    "\n",
    "    # AWS client errors\n",
    "    if ClientError and isinstance(exc, ClientError):\n",
    "        code = getattr(getattr(exc, \"response\", {}), \"get\", lambda *_: {})(\"Error\", {}).get(\"Code\")\n",
    "        if code:\n",
    "            return f\"AWS_{code}\"\n",
    "        return \"AWS_CLIENT_ERROR\"\n",
    "\n",
    "    # Persistent UI initialization errors\n",
    "    if isinstance(exc, ValueError):\n",
    "        if \"Persistent App UI did not become ready\" in msg:\n",
    "            return \"PERSISTENT_UI_NOT_READY\"\n",
    "        if \"No presigned URL\" in msg:\n",
    "            return \"PERSISTENT_UI_NO_PRESIGNED_URL\"\n",
    "        if \"No persistent UI ID\" in msg:\n",
    "            return \"PERSISTENT_UI_NO_ID\"\n",
    "\n",
    "    return \"UNKNOWN_ERROR\"\n",
    "\n",
    "def _compose_failed_status(endpoint_errors: Dict[str, List[Dict[str, Any]]], endpoint_attempted: Dict[str, bool], fallback_message: Optional[str] = None) -> str:\n",
    "    codes: List[str] = []\n",
    "    for errs in (endpoint_errors or {}).values():\n",
    "        for err in (errs or []):\n",
    "            code = err.get(\"code\")\n",
    "            if code and code not in codes:\n",
    "                codes.append(code)\n",
    "    if codes:\n",
    "        return f\"FAILED({','.join(codes[:MAX_ENDPOINT_FAILURES])})\"\n",
    "    if any(endpoint_attempted.values()):\n",
    "        if fallback_message:\n",
    "            safe = _truncate_message(fallback_message).replace('(', '[').replace(')', ']')\n",
    "            return f\"FAILED({safe})\"\n",
    "    \n",
    "        return \"FAILED(UNKNOWN)\"\n",
    "    return \"FAILED(NO_DATA)\"\n",
    "\n",
    "def _record_error(endpoint: str, fn: Any, exc: Exception, endpoint_errors: Dict[str, List]):\n",
    "    \"\"\"Records a structured error for a failed endpoint call.\"\"\"\n",
    "    now_iso = datetime.now().isoformat()\n",
    "    \n",
    "    # Determine a short code for the error type\n",
    "    if isinstance(exc, (requests.exceptions.ReadTimeout, requests.exceptions.Timeout)):\n",
    "        code = \"TIMEOUT\"\n",
    "    elif isinstance(exc, requests.exceptions.ConnectionError):\n",
    "        code = \"CONNECTION_ERROR\"\n",
    "    elif isinstance(exc, requests.exceptions.HTTPError):\n",
    "        code = f\"HTTP_{exc.response.status_code}\"\n",
    "    else:\n",
    "        code = \"UNKNOWN_ERROR\"\n",
    "\n",
    "    # Defensive: avoid masking original error recording if response properties fail\n",
    "    error_obj = {\n",
    "        'code': code,\n",
    "        'exception_type': type(exc).__name__,\n",
    "        'message': _truncate_message(str(exc)),\n",
    "        'api': getattr(fn, '__name__', 'unknown'),\n",
    "        'endpoint_key': endpoint,\n",
    "        'timestamp': now_iso\n",
    "    }\n",
    "\n",
    "    # Append with cap\n",
    "    lst = endpoint_errors.setdefault(endpoint, [])\n",
    "    if len(lst) < _MAX_ERRORS_PER_ENDPOINT:\n",
    "        lst.append(error_obj)\n",
    "\n",
    "def process_single_application(\n",
    "    app_id: str,\n",
    "    attempt_id: str,\n",
    "    shs_client: Any,\n",
    "    analyzer: Any,\n",
    "    cluster_id: str,\n",
    "    cluster_name: str,\n",
    "    endpoint_errors: Dict[str, List[Dict[str, Any]]],\n",
    "    endpoint_attempted: Dict[str, bool],\n",
    "    endpoint_skipped_reasons: Dict[str, List[str]], # Kept for consistent function signature\n",
    "    app_details: Dict[str, Any]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a single Spark application attempt by fetching its data concurrently.\n",
    "    This function uses a ThreadPoolExecutor to make parallel API calls for jobs, stages,\n",
    "    executors, and SQL queries to significantly speed up data retrieval.\n",
    "    \"\"\"\n",
    "    logger.info(\" -> Concurrently analyzing application %s, attempt %s\", app_id, attempt_id)\n",
    "    app_results = {\n",
    "        'applications': [], 'jobs': [], 'stages': [], 'tasks': [],\n",
    "        'sql_queries': [], 'executors': [], 'task_summaries': []\n",
    "    }\n",
    "\n",
    "    def add_and_append(data_list: List[Dict], result_key: str):\n",
    "        if not isinstance(data_list, list):\n",
    "            logger.warning(\"⚠️ Data provided to add_and_append for key '%s' is not a list, skipping.\", result_key)\n",
    "            return\n",
    "        for item in data_list:\n",
    "            item['application_id'] = app_id\n",
    "            item['attempt_id'] = attempt_id\n",
    "            item['cluster_id'] = cluster_id\n",
    "            item['cluster_name'] = cluster_name\n",
    "        app_results[result_key].extend(item for item in data_list if isinstance(item, dict))\n",
    "\n",
    "    def safe_call(endpoint_key: str, fn, *args, **kwargs):\n",
    "        endpoint_attempted[endpoint_key] = True\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Final failure for endpoint '%s' (attempt %s) after all retries: %s\", endpoint_key, attempt_id, str(e), exc_info=False)\n",
    "            _record_error(endpoint_key, fn, e, endpoint_errors)\n",
    "            return None\n",
    "\n",
    "    # --- Stage 1: Analyze Application Performance (using pre-fetched details) ---\n",
    "    try:\n",
    "        perf_analysis = analyzer.analyze_application_performance(app_details, attempt_id)\n",
    "        add_and_append([perf_analysis], 'applications')\n",
    "    except Exception as e:\n",
    "        logger.error(\"❌ Failed during 'applications' post-processing for attempt %s: %s\", attempt_id, str(e), exc_info=True)\n",
    "        _record_error('applications', analyzer.analyze_application_performance, e, endpoint_errors)\n",
    "\n",
    "    # --- Stage 2: Concurrently Fetch Primary Data Endpoints ---\n",
    "    fetched_data = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"SHS_Primary\") as executor:\n",
    "        future_to_endpoint = {\n",
    "            executor.submit(safe_call, 'jobs', shs_client.get_application_jobs, app_id, attempt_id, status='succeeded'): 'jobs',\n",
    "            executor.submit(safe_call, 'stages', shs_client.get_application_stages, app_id, attempt_id, status='complete'): 'stages',\n",
    "            executor.submit(safe_call, 'executors', shs_client.get_application_executors, app_id, attempt_id): 'executors',\n",
    "            executor.submit(safe_call, 'sql', shs_client.get_application_sql_queries, app_id, attempt_id): 'sql'\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_endpoint):\n",
    "            endpoint_key = future_to_endpoint[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    fetched_data[endpoint_key] = result\n",
    "            except Exception as e:\n",
    "                logger.error(\"❌ Exception retrieving result for endpoint '%s': %s\", endpoint_key, str(e), exc_info=True)\n",
    "\n",
    "    # --- Stage 3: Process Concurrently Fetched Data ---\n",
    "    if 'jobs' in fetched_data:\n",
    "        try:\n",
    "            job_analysis = analyzer.analyze_job_performance(fetched_data['jobs'])\n",
    "            add_and_append(job_analysis, 'jobs')\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed during 'jobs' processing for attempt %s: %s\", attempt_id, str(e), exc_info=True)\n",
    "            _record_error('jobs', analyzer.analyze_job_performance, e, endpoint_errors)\n",
    "\n",
    "    if 'executors' in fetched_data:\n",
    "        try:\n",
    "            executor_analysis = analyzer.analyze_executor_performance(fetched_data['executors'])\n",
    "            add_and_append(executor_analysis, 'executors')\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed during 'executors' processing for attempt %s: %s\", attempt_id, str(e), exc_info=True)\n",
    "            _record_error('executors', analyzer.analyze_executor_performance, e, endpoint_errors)\n",
    "\n",
    "    if 'sql' in fetched_data:\n",
    "        try:\n",
    "            sql_analysis = analyzer.analyze_sql_queries(fetched_data['sql'])\n",
    "            add_and_append(sql_analysis, 'sql_queries')\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed during 'sql' processing for attempt %s: %s\", attempt_id, str(e), exc_info=True)\n",
    "            _record_error('sql', analyzer.analyze_sql_queries, e, endpoint_errors)\n",
    "\n",
    "    # --- Stage 4: Process Stages and Concurrently Fetch Sub-tasks ---\n",
    "    if 'stages' in fetched_data:\n",
    "        stages_data = fetched_data['stages']\n",
    "        try:\n",
    "            stages_analysis = analyzer.analyze_stage_performance(stages_data)\n",
    "            add_and_append(stages_analysis, 'stages')\n",
    "        except Exception as e:\n",
    "            logger.error(\"❌ Failed during 'stages' post-processing for attempt %s: %s\", attempt_id, str(e), exc_info=True)\n",
    "            _record_error('stages', analyzer.analyze_stage_performance, e, endpoint_errors)\n",
    "\n",
    "        # Concurrently fetch tasks and summaries for all stages\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1, thread_name_prefix=\"SHS_SubTask\") as executor:\n",
    "            future_to_stage_task = {}\n",
    "            for stage_raw in stages_data:\n",
    "                stage_id = stage_raw.get('stageId')\n",
    "                stage_attempt_id = stage_raw.get('attemptId', 0)\n",
    "                if stage_id is None:\n",
    "                    continue\n",
    "                # Submit tasks and task_summaries calls for each stage\n",
    "                future_to_stage_task[executor.submit(safe_call, 'tasks', shs_client.get_stage_tasks, app_id, attempt_id, stage_id, stage_attempt_id)] = ('tasks', stage_id, stage_attempt_id)\n",
    "                if TASK_BINARY == 'true':\n",
    "                    future_to_stage_task[executor.submit(safe_call, 'task_summaries', shs_client.get_stage_task_summary, app_id, attempt_id, stage_id, stage_attempt_id)] = ('task_summaries', stage_id, stage_attempt_id)\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_stage_task):\n",
    "                endpoint_key, stage_id, stage_attempt_id = future_to_stage_task[future]\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if not result:\n",
    "                        continue\n",
    "                    \n",
    "                    if endpoint_key == 'tasks':\n",
    "                        for task in result:\n",
    "                            task['stage_id'] = stage_id\n",
    "                            task['stage_attempt_id'] = stage_attempt_id\n",
    "                        task_analysis = analyzer.analyze_task_performance(result)\n",
    "                        add_and_append(task_analysis, 'tasks')\n",
    "                    elif endpoint_key == 'task_summaries':\n",
    "                        result['stage_id'] = stage_id\n",
    "                        result['stage_attempt_id'] = stage_attempt_id\n",
    "                        summary_analysis = analyzer.analyze_task_summaries([result])\n",
    "                        add_and_append(summary_analysis, 'task_summaries')\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(\"❌ Exception retrieving sub-task result for endpoint '%s' stage '%s': %s\", endpoint_key, stage_id, str(e), exc_info=True)\n",
    "    else:\n",
    "        logger.warning(\"⚠️ No stage data returned from API for attempt %s. Skipping task and summary collection.\", attempt_id)\n",
    "    \n",
    "    return app_results\n",
    "\n",
    "def analyze_application_attempts(\n",
    "    app_id: str,\n",
    "    shs_client: Any,\n",
    "    analyzer: Any,\n",
    "    cluster_id: str,\n",
    "    cluster_name: str,\n",
    "    endpoint_errors: Dict[str, List[Dict[str, Any]]],\n",
    "    endpoint_attempted: Dict[str, bool],\n",
    "    endpoint_skipped_reasons: Dict[str, List[str]]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Orchestrates the analysis of a single application by finding all successful attempts\n",
    "    and processing each one individually.\n",
    "    \"\"\"\n",
    "    all_attempt_results = []\n",
    "    \n",
    "    # This is a safecall wrapper for use inside this orchestrator function\n",
    "    def orchestrator_safe_call(endpoint_key: str, fn, *args, **kwargs):\n",
    "        endpoint_attempted[endpoint_key] = True\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.error(\"API call for endpoint '%s' app '%s' failed: %s\", endpoint_key, app_id, str(e), exc_info=True)\n",
    "            _record_error(endpoint_key, fn, e, endpoint_errors)\n",
    "            return None\n",
    "\n",
    "    app_details = orchestrator_safe_call('applications', shs_client.get_application_details, app_id)\n",
    "\n",
    "    if not app_details or not app_details.get('attempts'):\n",
    "        logger.warning(\"Application %s has no details or attempts data. Skipping.\", app_id)\n",
    "        return []\n",
    "\n",
    "    attempts = app_details.get('attempts', [])\n",
    "    successful_attempts_og = [attempt for attempt in attempts if attempt.get('completed', False)]\n",
    "    successful_attempts = [attempt for attempt in attempts]\n",
    "    \n",
    "    logger.info(\"Found %d total attempts for app %s. Will analyze %d successful attempts.\", len(attempts), app_id, len(successful_attempts))\n",
    "    if not successful_attempts:\n",
    "        return []\n",
    "\n",
    "    for attempt in successful_attempts:\n",
    "        attempt_id = attempt.get('attemptId', 42)\n",
    "        #print(f\"!!!!! {attempt_id}\")\n",
    "        if not attempt_id:\n",
    "            continue\n",
    "        \n",
    "        attempt_result = process_single_application(\n",
    "            app_id=app_id,\n",
    "            attempt_id=attempt_id,\n",
    "            shs_client=shs_client,\n",
    "            analyzer=analyzer,\n",
    "            cluster_id=cluster_id,\n",
    "            cluster_name=cluster_name,\n",
    "            endpoint_errors=endpoint_errors,\n",
    "            endpoint_attempted=endpoint_attempted,\n",
    "            endpoint_skipped_reasons=endpoint_skipped_reasons,\n",
    "            app_details=app_details\n",
    "        )\n",
    "        if attempt_result:\n",
    "            all_attempt_results.append(attempt_result)\n",
    "            \n",
    "    return all_attempt_results\n",
    "\n",
    "def analyze_single_cluster(\n",
    "    cluster_info: Dict,\n",
    "    timeout_seconds: int,\n",
    "    max_applications: int,\n",
    "    spark_session: SparkSession\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Analyzes a single EMR cluster. This version now uses an orchestrator to analyze all successful attempts for each application.\"\"\"\n",
    "    cluster_id = cluster_info['cluster_id']\n",
    "    cluster_name = cluster_info['cluster_name']\n",
    "    spark_context_id = cluster_info['spark_context_id']\n",
    "    logger.info(\"Starting analysis for cluster: %s (%s)\", cluster_name, cluster_id)\n",
    "\n",
    "    results_aggregator = {\n",
    "        'cluster_id': cluster_id, 'cluster_name': cluster_name, 'spark_context_id': spark_context_id, 'status': cluster_info.get('status', 'UNKNOWN'),\n",
    "        'normalized_instance_hours': cluster_info.get('normalized_instance_hours', 0),\n",
    "        'applications': [], 'jobs': [], 'stages': [], 'tasks': [], 'sql_queries': [], 'executors': [], 'task_summaries': [],\n",
    "        'analysis_status': 'PENDING', 'error_message': ''\n",
    "    }\n",
    "    \n",
    "    tracked_endpoints = ['applications', 'jobs', 'stages', 'tasks', 'sql', 'executors', 'task_summaries']\n",
    "    endpoint_errors: Dict[str, List[Dict[str, Any]]] = {k: [] for k in tracked_endpoints}\n",
    "    endpoint_attempted: Dict[str, bool] = {k: False for k in tracked_endpoints}\n",
    "    endpoint_skipped_reasons: Dict[str, List[str]] = {k: [] for k in tracked_endpoints}\n",
    "    \n",
    "    # This is a safecall wrapper for use inside this orchestrator function\n",
    "    def cluster_safe_call(endpoint_key: str, fn, *args, **kwargs):\n",
    "        try:\n",
    "            # Circuit breaker logic (if implemented in client)\n",
    "            if hasattr(shs_client, 'should_skip') and shs_client.should_skip(endpoint_key):\n",
    "                logger.warning(\"Skipping endpoint %s due to prior failures\", endpoint_key)\n",
    "                endpoint_skipped_reasons.setdefault(endpoint_key, []).append(\"Skipped due to prior failures (circuit breaker)\")\n",
    "                return None\n",
    "        except UnboundLocalError:\n",
    "            pass  # shs_client not yet defined\n",
    "            \n",
    "        endpoint_attempted[endpoint_key] = True\n",
    "        try:\n",
    "            result = fn(*args, **kwargs)\n",
    "            try:\n",
    "                if hasattr(shs_client, 'record_success'):\n",
    "                    shs_client.record_success(endpoint_key)\n",
    "            except Exception: pass\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            logger.error(\"Endpoint '%s' call failed: %s\", endpoint_key, str(e), exc_info=True)\n",
    "            _record_error(endpoint_key, fn, e, endpoint_errors)\n",
    "            try:\n",
    "                if hasattr(shs_client, 'record_failure'):\n",
    "                    shs_client.record_failure(endpoint_key)\n",
    "            except Exception: pass\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        if spark_context_id is None:\n",
    "          cow = 'moo'\n",
    "        else:\n",
    "            dataplane_workspace_url = dbutils.secrets.get(scope=\"shscreds\", key=\"dpurl\")\n",
    "            base_url = f'{dataplane_workspace_url}/sparkui/{cluster_id}/driver-{spark_context_id}/'\n",
    "\n",
    "            cookies = {'DATAPLANE_DOMAIN_DBAUTH': dbutils.secrets.get(scope=\"shscreds\", key=\"cookies\"), 'PRIMARY_DOMAIN':'workspace'}\n",
    "\n",
    "            try:\n",
    "                session = requests.Session()\n",
    "                response = session.get(base_url, cookies=cookies, allow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                logger.info(\"✅ HTTP session established successfully (Status: %s)\", response.status_code)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(\"❌ Failed to establish HTTP session: %s\", str(e), exc_info=True)\n",
    "                raise\n",
    "\n",
    "            shs_client = SparkHistoryServerClient(base_url, session, cookies)\n",
    "            analyzer = SparkMetricsAnalyzer(spark_session)\n",
    "            \n",
    "            applications = cluster_safe_call('applications', shs_client.get_applications, limit=max_applications)\n",
    "\n",
    "            if not applications:\n",
    "                logger.warning(\"No applications found in Spark History Server for %s\", cluster_name)\n",
    "                results_aggregator['analysis_status'] = 'NO_APPLICATIONS(No data)'\n",
    "                results_aggregator['endpoint_errors'] = endpoint_errors\n",
    "                results_aggregator['endpoint_attempted'] = endpoint_attempted\n",
    "                results_aggregator['endpoint_skipped_reasons'] = endpoint_skipped_reasons\n",
    "                return results_aggregator\n",
    "\n",
    "            logger.info(\"Found %s applications to analyze in %s\", len(applications), cluster_name)\n",
    "            for app in applications:\n",
    "                app_id = app.get('id')\n",
    "                if not app_id:\n",
    "                    continue\n",
    "                \n",
    "                attempt_results_list = analyze_application_attempts(\n",
    "                    app_id, shs_client, analyzer, cluster_id, cluster_name,\n",
    "                    endpoint_errors, endpoint_attempted, endpoint_skipped_reasons\n",
    "                )\n",
    "                \n",
    "                for app_data in attempt_results_list:\n",
    "                    if app_data:\n",
    "                        for key in ['applications', 'jobs', 'stages', 'tasks', 'sql_queries', 'executors', 'task_summaries']:\n",
    "                            results_aggregator[key].extend(app_data.get(key, []))\n",
    "\n",
    "            # Finalize status\n",
    "            if not any(results_aggregator.get(key) for key in ['applications', 'jobs', 'stages', 'tasks', 'sql_queries', 'executors', 'task_summaries']):\n",
    "                results_aggregator['analysis_status'] = _compose_failed_status(endpoint_errors, endpoint_attempted, results_aggregator.get('error_message'))\n",
    "                if not results_aggregator['error_message']:\n",
    "                    results_aggregator['error_message'] = \"No data returned from SHS endpoints for any successful attempts\"\n",
    "                logger.warning(\"Cluster %s analysis produced no data across all endpoints.\", cluster_name)\n",
    "            else:\n",
    "                results_aggregator['analysis_status'] = 'COMPLETED'\n",
    "            \n",
    "            logger.info(\"Completed analysis for cluster: %s\", cluster_name)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to analyze cluster %s: %s\", cluster_name, str(e), exc_info=True)\n",
    "        results_aggregator['analysis_status'] = f\"FAILED({_classify_exception_code(e)})\"\n",
    "        results_aggregator['error_message'] = str(e)\n",
    "\n",
    "    results_aggregator['endpoint_errors'] = endpoint_errors\n",
    "    results_aggregator['endpoint_attempted'] = endpoint_attempted\n",
    "    results_aggregator['endpoint_skipped_reasons'] = endpoint_skipped_reasons\n",
    "    return results_aggregator\n",
    "\n",
    "def process_clusters_in_batches(\n",
    "    clusters_to_analyze: List[Dict],\n",
    "    batch_size: int,\n",
    "    batch_delay_seconds: int,\n",
    "    spark_session: Any\n",
    ") -> Tuple[List[Dict], List[Dict], List[Dict], List[Dict], List[Dict], List[Dict], List[Dict], List[Dict]]:\n",
    "    \"\"\"Process clusters in sequential batches to manage resources and API limits. This version builds a JSON-as-string status_details per cluster with endpoint-level OK/FAILED/SKIPPED.\"\"\"\n",
    "    all_results = {'applications': [], 'jobs': [], 'stages': [], 'tasks': [], 'sql_queries': [], 'executors': [], 'task_summaries': []}\n",
    "    cluster_summaries = []\n",
    "\n",
    "    def endpoint_status_summary(c: Dict[str, Any]) -> str:\n",
    "        key_map = {'applications': 'applications', 'jobs': 'jobs', 'stages': 'stages', 'tasks': 'tasks', 'sql': 'sql_queries', 'executors': 'executors', 'task_summaries': 'task_summaries'}\n",
    "        attempted = c.get('endpoint_attempted', {})\n",
    "        errors = c.get('endpoint_errors', {})\n",
    "        skipped = c.get('endpoint_skipped_reasons', {})\n",
    "        status_obj = {}\n",
    "        for ep, data_key in key_map.items():\n",
    "            has_data = len(c.get(data_key, [])) > 0\n",
    "            ep_attempted = bool(attempted.get(ep, False))\n",
    "            ep_errors = errors.get(ep, [])\n",
    "            ep_skips = skipped.get(ep, [])\n",
    "            if has_data:\n",
    "                status_obj[ep] = 'OK'\n",
    "            elif ep_attempted and len(ep_errors) > 0:\n",
    "                # Extract just the error codes from the error objects\n",
    "                error_codes = [err.get('code', 'UNKNOWN') for err in ep_errors[:MAX_ENDPOINT_FAILURES]]\n",
    "                status_obj[ep] = f\"FAILED({','.join(error_codes)})\"\n",
    "            else:\n",
    "                # Use a simplified reason\n",
    "                reason = ep_skips[0] if ep_skips else \"No data\"\n",
    "                # Truncate long reasons to keep it concise\n",
    "                if len(reason) > 30:\n",
    "                    reason = reason[:27] + \"...\"\n",
    "                status_obj[ep] = f\"SKIPPED({reason})\"\n",
    "        return json.dumps(status_obj, separators=(',', ':'))\n",
    "\n",
    "    total_batches = (len(clusters_to_analyze) + batch_size - 1) // batch_size\n",
    "    for i in range(0, len(clusters_to_analyze), batch_size):\n",
    "        batch_clusters = clusters_to_analyze[i:i + batch_size]\n",
    "        current_batch_num = (i // batch_size) + 1\n",
    "        logger.info(\"Processing batch %d/%d...\", current_batch_num, total_batches)\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=len(batch_clusters)) as executor:\n",
    "            future_to_cluster = {executor.submit(analyze_single_cluster, c_info, TIMEOUT_SECONDS, MAX_APPLICATIONS, spark_session): c_info for c_info in batch_clusters}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(future_to_cluster):\n",
    "                cluster_info = future_to_cluster[future]\n",
    "                try:\n",
    "                    cluster_results = future.result()\n",
    "                    if cluster_results:\n",
    "                        for key in all_results.keys():\n",
    "                            all_results[key].extend(cluster_results.get(key, []))\n",
    "                        \n",
    "                        cluster_summaries.append({\n",
    "                            'cluster_id': cluster_results['cluster_id'],\n",
    "                            'cluster_name': cluster_results['cluster_name'],\n",
    "                            'status': cluster_results['status'],\n",
    "                            'spark_context_id': cluster_results['spark_context_id'],\n",
    "                            'analysis_status': cluster_results['analysis_status'],\n",
    "                            'status_details': endpoint_status_summary(cluster_results),\n",
    "                            'normalized_instance_hours': cluster_results.get('normalized_instance_hours', 0),\n",
    "                            'total_applications': len(cluster_results['applications']),\n",
    "                            'total_jobs': len(cluster_results['jobs']),\n",
    "                            'total_stages': len(cluster_results['stages']),\n",
    "                            'total_tasks': len(cluster_results['tasks']),\n",
    "                            'total_sql_queries': len(cluster_results['sql_queries']),\n",
    "                            'total_executors': len(cluster_results['executors']),\n",
    "                            'total_task_summaries': len(cluster_results['task_summaries']),\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    logger.error(\"Error processing results for cluster %s: %s\", cluster_info['cluster_id'], str(e), exc_info=True)\n",
    "                    cluster_summaries.append({\n",
    "                        'cluster_id': cluster_info['cluster_id'],\n",
    "                        'cluster_name': cluster_info['cluster_name'],\n",
    "                        'spark_context_id': cluster_info['spark_context_id'],\n",
    "                        'status': 'FAILED_PROCESSING',\n",
    "                        'analysis_status': f\"FAILED({_classify_exception_code(e)})\",\n",
    "                        'status_details': json.dumps({'error': _truncate_message(str(e))}),\n",
    "                        'normalized_instance_hours': cluster_info.get('normalized_instance_hours', 0),\n",
    "                        'total_applications': 0, 'total_jobs': 0, 'total_stages': 0,\n",
    "                        'total_tasks': 0, 'total_sql_queries': 0, 'total_executors': 0, 'total_task_summaries': 0\n",
    "                    })\n",
    "\n",
    "        if current_batch_num < total_batches:\n",
    "            logger.info(\"Waiting %d seconds between batches...\", batch_delay_seconds)\n",
    "            time.sleep(batch_delay_seconds)\n",
    "            \n",
    "    return all_results['applications'], all_results['jobs'], all_results['stages'], all_results['tasks'], all_results['sql_queries'], all_results['executors'], all_results['task_summaries'], cluster_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a1b4ea-bf42-493c-af8a-05f3a286fded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "186db167-6608-405b-9d69-33dbf0764139",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Execution"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main_analysis():\n",
    "    \"\"\"Main function to drive the EMR cluster analysis.\"\"\"\n",
    "    try:\n",
    "        # Step 1: Discover Clusters\n",
    "        dbx_discovery = DBXClusterDiscovery()\n",
    "        total_clusters_discovered = 0\n",
    "        logger.info(\"Discovering DBX clusters based on specified criteria...\")\n",
    "        discovered_clusters = dbx_discovery.discover_clusters(\n",
    "            max_clusters=MAX_CLUSTERS\n",
    "        )\n",
    "        total_clusters_discovered = len(discovered_clusters)\n",
    "            \n",
    "        \n",
    "        logger.info(\"Will analyze %s clusters.\", total_clusters_discovered)\n",
    "\n",
    "        # Step 3: Process clusters in batches\n",
    "        all_apps, all_jobs, all_stages, all_tasks, all_sql, all_execs, all_task_sums, summaries = process_clusters_in_batches(\n",
    "            discovered_clusters, BATCH_SIZE, BATCH_DELAY_SECONDS, spark\n",
    "        )\n",
    "\n",
    "        # Step 4: Create DataFrames\n",
    "        logger.info(\"Creating analysis DataFrames...\")\n",
    "        analyzer = SparkMetricsAnalyzer(spark)\n",
    "        apps_df, jobs_df, stages_df, tasks_df, sql_df, exec_df, task_sum_df = analyzer.create_dynamic_dataframes(\n",
    "            all_apps, all_jobs, all_stages, all_tasks, all_sql, all_execs, all_task_sums\n",
    "        )\n",
    "\n",
    "        cluster_summary_df = None\n",
    "        if summaries:\n",
    "            summary_schema = StructType([\n",
    "                StructField(\"cluster_id\", StringType(), True), StructField(\"cluster_name\", StringType(), True), StructField(\"spark_context_id\", StringType(), True),\n",
    "                StructField(\"status\", StringType(), True), StructField(\"analysis_status\", StringType(), True),\n",
    "                StructField(\"status_details\", StringType(), True),\n",
    "                StructField(\"normalized_instance_hours\", IntegerType(), True),\n",
    "                StructField(\"total_applications\", IntegerType(), True), StructField(\"total_jobs\", IntegerType(), True),\n",
    "                StructField(\"total_stages\", IntegerType(), True), StructField(\"total_tasks\", IntegerType(), True),\n",
    "                StructField(\"total_sql_queries\", IntegerType(), True), StructField(\"total_executors\", IntegerType(), True),\n",
    "                StructField(\"total_task_summaries\", IntegerType(), True)\n",
    "            ])\n",
    "            cluster_summary_df = spark.createDataFrame(summaries, schema=summary_schema)\n",
    "\n",
    "        # Step 5: Final Summary\n",
    "        total_clusters_analyzed = len([c for c in (summaries or []) if c.get('analysis_status') == 'COMPLETED'])\n",
    "        \n",
    "        # Calculate clusters_fully_analyzed - clusters with data from all major endpoints\n",
    "        clusters_fully_analyzed = 0\n",
    "        if summaries:\n",
    "            for cluster_summary in summaries:\n",
    "                # A cluster is considered fully analyzed if it has data from all major endpoints\n",
    "                has_all_data = (\n",
    "                    cluster_summary.get('total_applications', 0) > 0 and\n",
    "                    cluster_summary.get('total_jobs', 0) > 0 and\n",
    "                    cluster_summary.get('total_stages', 0) > 0 and\n",
    "                    cluster_summary.get('total_tasks', 0) > 0 and\n",
    "                    cluster_summary.get('total_sql_queries', 0) > 0 and\n",
    "                    cluster_summary.get('total_executors', 0) > 0 and\n",
    "                    cluster_summary.get('total_task_summaries', 0) > 0\n",
    "                )\n",
    "                if has_all_data:\n",
    "                    clusters_fully_analyzed += 1\n",
    "        \n",
    "        # Calculate success rate\n",
    "        cluster_analysis_success_rate = 0.0\n",
    "        if total_clusters_analyzed > 0:\n",
    "            cluster_analysis_success_rate = round((clusters_fully_analyzed / total_clusters_analyzed) * 100.0, 2)\n",
    "        \n",
    "        final_summary = {\n",
    "            'clusters_discovered_count': total_clusters_discovered,\n",
    "            'clusters_extracted_count': total_clusters_analyzed,\n",
    "            'clusters_fully_analyzed_count': clusters_fully_analyzed,\n",
    "            'cluster_analysis_success_rate %': cluster_analysis_success_rate,\n",
    "            'total_applications': len(all_apps),\n",
    "            'total_jobs': len(all_jobs),\n",
    "            'total_stages': len(all_stages),\n",
    "            'total_tasks': len(all_tasks),\n",
    "            'total_sql_queries': len(all_sql),\n",
    "            'total_executors': len(all_execs),\n",
    "            'total_task_summaries': len(all_task_sums)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"cluster_summaries_df\": cluster_summary_df,\n",
    "            \"applications_df\": apps_df,\n",
    "            \"jobs_df\": jobs_df,\n",
    "            \"stages_df\": stages_df,\n",
    "            \"tasks_df\": tasks_df,\n",
    "            \"sql_df\": sql_df,\n",
    "            \"executors_df\": exec_df,\n",
    "            \"task_summaries_df\": task_sum_df,\n",
    "            \"summary\": final_summary\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(\"Main analysis failed: %s\", str(e), exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Execute main analysis\n",
    "results = main_analysis()\n",
    "\n",
    "# Display summary and make DataFrames available\n",
    "if results:\n",
    "    cluster_summaries_df = results.get('cluster_summaries_df')\n",
    "    applications_df = results.get('applications_df')\n",
    "    jobs_df = results.get('jobs_df')\n",
    "    stages_df = results.get('stages_df')\n",
    "    tasks_df = results.get('tasks_df')\n",
    "    sql_df = results.get('sql_df')\n",
    "    executors_df = results.get('executors_df')\n",
    "    task_summaries_df = results.get('task_summaries_df')\n",
    "    analysis_summary = results.get('summary', {})\n",
    "    \n",
    "    print(\"-\" * 100)\n",
    "    print(\"EMR SPARK HISTORY ANALYSIS COMPLETED!\")\n",
    "    print(\"-\" * 100)\n",
    "    for key, value in analysis_summary.items():\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "    print(\"\\nDataFrames available for analysis: cluster_summaries_df, applications_df, jobs_df, stages_df, tasks_df, sql_df, executors_df, task_summaries_df\")\n",
    "\n",
    "    # Write outputs as managed Delta tables in the configured catalog/schema\n",
    "    if ENVIRONMENT == \"prod\":\n",
    "        logger.info(\"Writing analysis results to Delta tables in %s.%s\", CATALOG_NAME, SCHEMA_NAME)\n",
    "        try:\n",
    "            run_ts = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\n",
    "            for df_name, df_instance in results.items():\n",
    "                if df_name.endswith('_df') and df_instance:\n",
    "                    table_name = df_name.replace('_df', '')\n",
    "                    versioned_table_name = f\"{table_name}_{run_ts}\"\n",
    "                    full_table_name = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{versioned_table_name}\"\n",
    "                    (\n",
    "                        df_instance.write\n",
    "                        .format(\"delta\")\n",
    "                        .mode(\"errorifexists\")\n",
    "                        .option(\"overwriteSchema\", \"true\")\n",
    "                        .saveAsTable(full_table_name)\n",
    "                    )\n",
    "            logger.info(\"All analysis results successfully written to Delta tables.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ ANALYSIS FAILED: {str(e)} - check the logs above for detailed error information.\")\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    handler.flush()\n",
    "    handler.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1facb261-6c5b-4439-b24a-a29b691b762d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Table Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6726713a-d317-47f3-b78c-b5ea9f48e83e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analysis Summary"
    }
   },
   "outputs": [],
   "source": [
    "# The analysis_summary dictionary contains the final counts.\n",
    "if 'analysis_summary' in locals() and analysis_summary:\n",
    "    display(spark.createDataFrame([analysis_summary]))\n",
    "else:\n",
    "    print(\"Analysis summary is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6506f636-d50b-4144-a21c-e09f5ad65758",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cluster Summaries"
    }
   },
   "outputs": [],
   "source": [
    "if 'cluster_summaries_df' in locals() and cluster_summaries_df:\n",
    "    display(cluster_summaries_df)\n",
    "else:\n",
    "    print(\"Cluster summaries DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640b1a81-07db-4ab9-8053-55359f2a419b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Applications"
    }
   },
   "outputs": [],
   "source": [
    "if 'applications_df' in locals() and applications_df:\n",
    "    display(applications_df)\n",
    "else:\n",
    "    print(\"Applications DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5182cb75-6dd9-455d-8043-1c5e22c96573",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Executors"
    }
   },
   "outputs": [],
   "source": [
    "if 'executors_df' in locals() and executors_df:\n",
    "    display(executors_df)\n",
    "else:\n",
    "    print(\"Executors DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ecaf6af-ee1d-4275-9b03-815c91634fa8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Jobs"
    }
   },
   "outputs": [],
   "source": [
    "if 'jobs_df' in locals() and jobs_df:\n",
    "    display(jobs_df)\n",
    "else:\n",
    "    print(\"Jobs DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3f6161-1939-492a-8aff-a4654b44b543",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stages"
    }
   },
   "outputs": [],
   "source": [
    "if 'stages_df' in locals() and stages_df:\n",
    "    display(stages_df)\n",
    "else:\n",
    "    print(\"Stages DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0038561-e12b-4fa7-8529-dbe7592d20b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tasks"
    }
   },
   "outputs": [],
   "source": [
    "if 'tasks_df' in locals() and tasks_df:\n",
    "    display(tasks_df)\n",
    "else:\n",
    "    print(\"Tasks DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbd2b65-b648-4370-a6ed-991c549e6fa2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Task Summaries"
    }
   },
   "outputs": [],
   "source": [
    "if 'task_summaries_df' in locals() and task_summaries_df:\n",
    "    display(task_summaries_df)\n",
    "else:\n",
    "    print(\"Task summaries DataFrame is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df952512-8447-4280-a3f0-71fc2fd6cc24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SQL Queries"
    }
   },
   "outputs": [],
   "source": [
    "if 'sql_df' in locals() and sql_df:\n",
    "    display(sql_df)\n",
    "else:\n",
    "    print(\"SQL queries DataFrame is not available.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dbx_spark_profiler",
   "widgets": {
    "batch_delay_seconds": {
     "currentValue": "2",
     "nuid": "7ce941f5-414d-4601-ad8e-c8552abf7fb0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "2",
      "label": "Delay Between Batches (seconds)",
      "name": "batch_delay_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "2",
      "label": "Delay Between Batches (seconds)",
      "name": "batch_delay_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "batch_size": {
     "currentValue": "10",
     "nuid": "8ad4b272-74e2-468f-81ce-fb8935f28657",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": "Batch Size (clusters to process concurrently)",
      "name": "batch_size",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": "Batch Size (clusters to process concurrently)",
      "name": "batch_size",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog_name": {
     "currentValue": "tcarter",
     "nuid": "1ffdea02-cb30-4056-8fd1-cd280b5afae6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "environment": {
     "currentValue": "prod",
     "nuid": "d17dc37a-22f0-4e99-a9d3-ea9bd13e0661",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Environment (dev/prod)",
      "name": "environment",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "include_tasks": {
     "currentValue": "false",
     "nuid": "d150d2b5-cf4f-4808-a3f6-73157f371b02",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Set to true if you want to include task level metrics",
      "name": "include_tasks",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "Set to true if you want to include task level metrics",
      "name": "include_tasks",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_applications": {
     "currentValue": "10",
     "nuid": "a0fc4e8d-e870-4b31-b9e4-4edaf0c0d82e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": "Max Applications to Analyze per Cluster",
      "name": "max_applications",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_clusters": {
     "currentValue": "200",
     "nuid": "ae4a018a-4023-457c-9501-1efe8ef01f7e",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "200",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "200",
      "label": "Max Clusters to Analyze",
      "name": "max_clusters",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "max_endpoint_failures": {
     "currentValue": "3",
     "nuid": "84792ae8-6be4-4c27-a8cb-226c6169d25a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "3",
      "label": "Max Endpoint Failures per Endpoint Type",
      "name": "max_endpoint_failures",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "3",
      "label": "Max Endpoint Failures per Endpoint Type",
      "name": "max_endpoint_failures",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "spark_observability",
     "nuid": "3311fc30-37b2-4022-8bb6-cc2b7087657a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "timeout_seconds": {
     "currentValue": "300",
     "nuid": "1fec60e2-0c09-48ce-b4e8-7b093acb40ba",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "300",
      "label": "Request Timeout (seconds)",
      "name": "timeout_seconds",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume_name": {
     "currentValue": "profiler_logs_volume",
     "nuid": "5e75133f-b2bd-43b6-bfde-bd569a015c74",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "profiler_logs_volume",
      "label": "Volume for logs",
      "name": "volume_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "profiler_logs_volume",
      "label": "Volume for logs",
      "name": "volume_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

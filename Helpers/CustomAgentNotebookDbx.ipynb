{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d438673-cc46-4b5f-8dbd-ee1ef36e06af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, base64, json, re\n",
    "from databricks.sdk import WorkspaceClient \n",
    "\n",
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Catalog (required)\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Schema\")\n",
    "CATALOG_NAME = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"schema_name\").strip()\n",
    "\n",
    "w = WorkspaceClient()\n",
    "CP_URL_RAW = w.secrets.get_secret(scope=\"shscreds\", key=\"cpurl\").value\n",
    "CP_URL = base64.b64decode(CP_URL_RAW).decode('utf-8').strip().rstrip('/')\n",
    "DP_URL_RAW = w.secrets.get_secret(scope=\"shscreds\", key=\"dpurl\").value\n",
    "DP_URL = base64.b64decode(CP_URL_RAW).decode('utf-8').strip().rstrip('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0233c735-6d6e-4d3f-8102-2c5ab1fc8c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq backoff databricks-openai openai-agents uv databricks-agents mlflow-skinny[databricks]\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "246cce6b-4b80-4233-9edc-83efcf2856e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 1. Get Variables\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\").strip()\n",
    "FUNC_NAME = f\"{CATALOG}.{SCHEMA}.py_listshssqlraw\"\n",
    "TOOL_NAME_CLEAN = FUNC_NAME.replace('.', '__')\n",
    "\n",
    "# 2. Generate the Agent Code\n",
    "agent_code_template = \"\"\"\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from typing import Any, Callable, Generator, Optional, List\n",
    "from uuid import uuid4\n",
    "\n",
    "import mlflow\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from mlflow.entities import SpanType\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from unitycatalog.ai.core.base import get_uc_function_client\n",
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "# Configuration\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-opus-4-6\"\n",
    "MAX_TOKENS = 125000\n",
    "SYSTEM_PROMPT = \\\"\\\"\\\"You are a senior spark performance engineer. Use the get_raw_sql_metrics_full(<cluster_id> ) to retrieve the cluster's Spark SQL execution plan JSON blob for a Spark application/job. Please analyze the execution plan and provide a deep, detailed analysis for the job run, including any bottlenecks found and performance recommendations/optimizations.\\\"\\\"\\\"\n",
    "\n",
    "# Tool Helper Classes\n",
    "class ToolInfo(BaseModel):\n",
    "    name: str\n",
    "    spec: dict\n",
    "    exec_fn: Callable\n",
    "\n",
    "class ToolCallingAgent(ResponsesAgent):\n",
    "    def __init__(self, llm_endpoint: str):\n",
    "        self.llm_endpoint = llm_endpoint\n",
    "        \n",
    "        # Initialize standard client for LLM\n",
    "        self.workspace_client = WorkspaceClient()\n",
    "        self.model_serving_client: OpenAI = (\n",
    "            self.workspace_client.serving_endpoints.get_open_ai_client()\n",
    "        )\n",
    "        self._tools_dict = self._load_tools()\n",
    "\n",
    "    def _load_tools(self) -> dict[str, ToolInfo]:\n",
    "        print(\"Initializing Tools via Manual Spec...\")\n",
    "        \n",
    "        # 1. DEFINE THE TOOL MANUALLY\n",
    "        tool_name = \"REPLACE_TOOL_NAME_CLEAN\"\n",
    "        \n",
    "        tool_spec = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool_name,\n",
    "                \"description\": \"Retrieves Spark SQL execution plan metrics for a given cluster.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"cluster_id\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The Cluster ID to analyze\"\n",
    "                        }\n",
    "                        # HIDDEN: pat_token AND cp_url are omitted here\n",
    "                    },\n",
    "                    \"required\": [\"cluster_id\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # 2. DEFINE EXECUTION WITH EXPLICIT CLIENT\n",
    "        def exec_fn(**kwargs):\n",
    "            udf_name = \"REPLACE_FUNC_NAME\"\n",
    "            print(f\"Executing {udf_name}...\")\n",
    "            \n",
    "            # A. Retrieve Secrets/Env Vars\n",
    "            pat_token = os.environ.get(\"USER_PAT_OVERRIDE\")\n",
    "            cp_url = os.environ.get(\"USER_CP_URL_OVERRIDE\")\n",
    "\n",
    "            if not pat_token:\n",
    "                return json.dumps({\"error\": \"USER_PAT_OVERRIDE not found.\"})\n",
    "            if not cp_url:\n",
    "                return json.dumps({\"error\": \"USER_CP_URL_OVERRIDE not found.\"})\n",
    "\n",
    "            # B. Inject tokens into arguments\n",
    "            # The UDF now expects these 3 args: cluster_id, pat_token, cp_url\n",
    "            kwargs[\"pat_token\"] = pat_token\n",
    "            kwargs[\"cp_url\"] = cp_url\n",
    "\n",
    "            # C. Create Client\n",
    "            # We explicitly pass the host (cp_url) and token to ensure the client connects correctly\n",
    "            client = DatabricksFunctionClient(\n",
    "                client=WorkspaceClient(host=cp_url, token=pat_token)\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                # D. Execute\n",
    "                res = client.execute_function(udf_name, kwargs)\n",
    "                if res.error:\n",
    "                    return f\"Error executing function: {res.error}\"\n",
    "                \n",
    "                # --- UX FIX: Compact & Prefix Output ---\n",
    "                result_val = res.value\n",
    "                if not isinstance(result_val, str):\n",
    "                    result_val = json.dumps(result_val)\n",
    "                    \n",
    "                compact_json = result_val.replace('\\\\n', '').replace('\\\\r', '')\n",
    "                final_output = f\"SPARK_METRICS_DATA: {compact_json}\"\n",
    "                \n",
    "                return final_output\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"Exception during execution: {str(e)}\"\n",
    "        \n",
    "        return {\n",
    "            tool_name: ToolInfo(name=tool_name, spec=tool_spec, exec_fn=exec_fn)\n",
    "        }\n",
    "\n",
    "    def get_tool_specs(self) -> list[dict]:\n",
    "        return [tool_info.spec for tool_info in self._tools_dict.values()]\n",
    "\n",
    "    @mlflow.trace(span_type=SpanType.TOOL)\n",
    "    def execute_tool(self, tool_name: str, args: dict) -> Any:\n",
    "        return self._tools_dict[tool_name].exec_fn(**args)\n",
    "\n",
    "    def call_llm(self, messages: list[dict[str, Any]]) -> Generator[dict[str, Any], None, None]:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\"PydanticSerializationUnexpectedValue\")\n",
    "            for chunk in self.model_serving_client.chat.completions.create(\n",
    "                model=self.llm_endpoint,\n",
    "                messages=to_chat_completions_input(messages),\n",
    "                tools=self.get_tool_specs(),\n",
    "                stream=True,\n",
    "                max_tokens=MAX_TOKENS \n",
    "            ):\n",
    "                chunk_dict = chunk.to_dict()\n",
    "                if len(chunk_dict.get(\"choices\", [])) > 0:\n",
    "                    yield chunk_dict\n",
    "\n",
    "    def handle_tool_call(self, tool_call: dict[str, Any], messages: list[dict[str, Any]]) -> ResponsesAgentStreamEvent:\n",
    "        try:\n",
    "            args = json.loads(tool_call.get(\"arguments\"))\n",
    "        except Exception:\n",
    "            args = {}\n",
    "        result = str(self.execute_tool(tool_name=tool_call[\"name\"], args=args))\n",
    "        tool_call_output = self.create_function_call_output_item(tool_call[\"call_id\"], result)\n",
    "        messages.append(tool_call_output)\n",
    "        return ResponsesAgentStreamEvent(type=\"response.output_item.done\", item=tool_call_output)\n",
    "\n",
    "    def call_and_run_tools(self, messages: list[dict[str, Any]], max_iter: int = 10) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        for _ in range(max_iter):\n",
    "            last_msg = messages[-1]\n",
    "            if last_msg.get(\"role\", None) == \"assistant\":\n",
    "                return\n",
    "            elif last_msg.get(\"type\", None) == \"function_call\":\n",
    "                yield self.handle_tool_call(last_msg, messages)\n",
    "            else:\n",
    "                yield from output_to_responses_items_stream(\n",
    "                    chunks=self.call_llm(messages), aggregator=messages\n",
    "                )\n",
    "        yield ResponsesAgentStreamEvent(\n",
    "            type=\"response.output_item.done\",\n",
    "            item=self.create_text_output_item(\"Max iterations reached. Stopping.\", str(uuid4())),\n",
    "        )\n",
    "\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        session_id = request.custom_inputs.get(\"session_id\") if request.custom_inputs else None\n",
    "        if not session_id and request.context:\n",
    "            session_id = request.context.conversation_id\n",
    "        if session_id:\n",
    "            mlflow.update_current_trace(metadata={\"mlflow.trace.session\": session_id})\n",
    "\n",
    "        outputs = [event.item for event in self.predict_stream(request) if event.type == \"response.output_item.done\"]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    def predict_stream(self, request: ResponsesAgentRequest) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        session_id = request.custom_inputs.get(\"session_id\") if request.custom_inputs else None\n",
    "        if not session_id and request.context:\n",
    "            session_id = request.context.conversation_id\n",
    "        if session_id:\n",
    "            mlflow.update_current_trace(metadata={\"mlflow.trace.session\": session_id})\n",
    "\n",
    "        messages = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        if SYSTEM_PROMPT:\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": SYSTEM_PROMPT})\n",
    "        yield from self.call_and_run_tools(messages=messages)\n",
    "\n",
    "# Log the model using MLflow\n",
    "AGENT = ToolCallingAgent(llm_endpoint=LLM_ENDPOINT_NAME)\n",
    "mlflow.models.set_model(AGENT)\n",
    "\"\"\"\n",
    "\n",
    "# 3. Perform the substitution\n",
    "final_agent_code = agent_code_template.replace(\"REPLACE_FUNC_NAME\", FUNC_NAME)\n",
    "final_agent_code = final_agent_code.replace(\"REPLACE_TOOL_NAME_CLEAN\", TOOL_NAME_CLEAN)\n",
    "\n",
    "# 4. Write the file\n",
    "with open(\"agent.py\", \"w\") as f:\n",
    "    f.write(final_agent_code)\n",
    "\n",
    "print(f\"✅ Generated agent.py with CP_URL injection for: {FUNC_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826e9c75-237b-4323-b3fd-3ab026d16885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models.resources import DatabricksFunction, DatabricksServingEndpoint\n",
    "\n",
    "# 1. Define Resources Manually\n",
    "# We list the endpoint and the function so Model Serving knows they are dependencies\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=\"databricks-claude-opus-4-6\"),\n",
    "    # We point to the inner function we are using\n",
    "    DatabricksFunction(function_name=f\"{CATALOG}.{SCHEMA}.py_listshssqlraw\")\n",
    "]\n",
    "\n",
    "# 2. FAST Input Example\n",
    "# We use a simple greeting so the Agent DOES NOT call the heavy tool during logging.\n",
    "# This generates the same schema (String -> String) but runs instantly.\n",
    "input_example = {\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Hello! Are you ready to analyze Spark logs?\"}],\n",
    "    \"custom_inputs\": {\"session_id\": \"fast-logging-test\"}\n",
    "}\n",
    "\n",
    "print(\"Logging model (fast mode)...\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        input_example=input_example,\n",
    "        pip_requirements=[\n",
    "            \"databricks-openai\",\n",
    "            \"databricks-sdk\",\n",
    "            \"pydantic\",\n",
    "            \"mlflow\",\n",
    "            \"backoff\",\n",
    "            \"unitycatalog-ai\" \n",
    "        ],\n",
    "        resources=resources,\n",
    "    )\n",
    "\n",
    "print(f\"✅ Model logged successfully at: {logged_agent_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1abde4-ae07-4741-be94-452bb6fab190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the model back into the current session to test it\n",
    "# This bypasses the auth issues of the isolated 'uv' environment\n",
    "print(f\"Testing model from run: {logged_agent_info.run_id}\")\n",
    "loaded_model = mlflow.pyfunc.load_model(f\"runs:/{logged_agent_info.run_id}/agent\")\n",
    "\n",
    "# Run a live test\n",
    "response = loaded_model.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Hello! Are you ready?\"}], \n",
    "    \"custom_inputs\": {\"session_id\": \"final-validation\"}\n",
    "})\n",
    "\n",
    "print(\"\\nResponse from Agent:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce13548-dc4a-495c-9173-a343477c6bcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\").strip()\n",
    "\n",
    "\n",
    "model_name = \"spark_observability_agent_dbx\"\n",
    "UC_MODEL_NAME = f\"{CATALOG}.{SCHEMA}.{model_name}\"\n",
    "\n",
    "# register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=UC_MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"✅ Model registered: {UC_MODEL_NAME} (Version {uc_registered_model_info.version})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505d7974-c267-41fd-b8eb-63d41726c05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# 1. SETUP CONFIG\n",
    "CATALOG = dbutils.widgets.get(\"catalog_name\").strip()\n",
    "SCHEMA = dbutils.widgets.get(\"schema_name\").strip()\n",
    "model_name = \"spark_observability_agent_dbx\"\n",
    "UC_MODEL_FULL_NAME = f\"{CATALOG}.{SCHEMA}.{model_name}\"\n",
    "\n",
    "# 2. CRITICAL STEP: INJECT THE SECRET LOCALLY\n",
    "# The Agent code now looks for \"USER_PAT_OVERRIDE\" in os.environ.\n",
    "# We fetch the real PAT from your secret scope and set it here for this session.\n",
    "try:\n",
    "    # Fetch the secret (returns the actual string value in the notebook)\n",
    "    dev_pat = dbutils.secrets.get(scope=\"shscreds\", key=\"pat\")\n",
    "    cpurl = dbutils.secrets.get(scope=\"shscreds\", key=\"cpurl\")\n",
    "    \n",
    "    # Set the environment variable so agent.py can read it\n",
    "    os.environ[\"USER_PAT_OVERRIDE\"] = dev_pat\n",
    "    os.environ[\"USER_CP_URL_OVERRIDE\"] = cpurl\n",
    "    print(\"✅ Successfully injected USER_PAT_OVERRIDE into local environment.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not fetch secret. Agent execution might fail if UDF requires auth.\\nError: {e}\")\n",
    "\n",
    "# 3. LOAD MODEL\n",
    "# Replace '3' with the actual version you want to test if needed\n",
    "model_uri = f\"models:/{UC_MODEL_FULL_NAME}/1\"\n",
    "print(f\"Loading model from UC: {model_uri}\")\n",
    "\n",
    "uc_agent = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# 4. PREDICT\n",
    "# Now when the agent runs 'exec_fn', it will find the token in os.environ\n",
    "print(\"Running prediction...\")\n",
    "response = uc_agent.predict({\n",
    "    \"input\": [{\"role\": \"user\", \"content\": \"Analyze performance for cluster 0205-013230-2hrzofqr\"}],\n",
    "    \"custom_inputs\": {\"session_id\": \"uc-test-local-01\"}\n",
    "})\n",
    "\n",
    "print(\"Success!\")\n",
    "# print(response[0]) # Uncomment to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf019d75-be96-4306-bba1-954e583e5426",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 1. Run the prediction (assuming you haven't already)\n",
    "# response = local_agent.predict(request) \n",
    "\n",
    "# 2. Parse the complex Agent response\n",
    "final_markdown_text = \"\"\n",
    "\n",
    "# The 'response' object is a dictionary containing a list of events/messages\n",
    "if isinstance(response, dict) and \"output\" in response:\n",
    "    for item in response[\"output\"]:\n",
    "        # We only care about messages from the assistant (the AI)\n",
    "        if item.get(\"type\") == \"message\" and item.get(\"role\") == \"assistant\":\n",
    "            # Extract the actual text content from the message\n",
    "            for content_item in item.get(\"content\", []):\n",
    "                if content_item.get(\"type\") == \"output_text\":\n",
    "                    final_markdown_text += content_item.get(\"text\", \"\")\n",
    "\n",
    "# 3. Display formatted Markdown\n",
    "print(\"=\"*50)\n",
    "print(\"FORMATTED AGENT RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "display(Markdown(final_markdown_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce18c202-cfc4-49a6-8aaa-62c2d6fd9699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# This reference tells Model Serving to fetch the secret securely \n",
    "# and expose it as an environment variable inside the container.\n",
    "SECRET_PATH = \"{{secrets/shscreds/pat}}\"\n",
    "SECRET_CP_URL = \"{{secrets/shscreds/cpurl}}\"\n",
    "\n",
    "print(f\"Deploying model version {uc_registered_model_info.version}\")\n",
    "\n",
    "agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version, \n",
    "    tags={\"endpointSource\": \"playground\"},\n",
    "    environment_vars={\n",
    "        \"USER_PAT_OVERRIDE\": SECRET_PATH,\n",
    "        \"USER_CP_URL_OVERRIDE\": SECRET_CP_URL\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3239668367622564,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "CustomAgentNotebookDbx",
   "widgets": {
    "catalog_name": {
     "currentValue": "users",
     "nuid": "d4634225-f8ff-489c-b61e-758d49bc7f97",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Catalog (required)",
      "name": "catalog_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema_name": {
     "currentValue": "roberto_salcido",
     "nuid": "1393ba1c-9450-4535-859e-36d6825a84ee",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Schema",
      "name": "schema_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
